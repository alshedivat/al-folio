<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>MEMM | Ariel Goodwin</title> <meta name="author" content="Ariel Goodwin"/> <meta name="description" content="Using the Maximum Entropy on the Mean Method (MEMM) to solve ill-posed inverse problems."/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üßÆ</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://arielgoodwin.github.io/projects/2_project/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Ariel¬†</span>Goodwin</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item"><a class="nav-link" href="/assets/pdf/cv.pdf" target="_blank">CV</a></li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">Talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/notes/">Notes</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">MEMM</h1> <p class="post-description">Using the Maximum Entropy on the Mean Method (MEMM) to solve ill-posed inverse problems.</p> </header> <article> <p>Given a linear map (i.e., a matrix) \(C \colon \mathbb{R}^n \to \mathbb{R}^m\) and a vector \(b\in \mathbb{R}^m\) which represents our observations, the canonical linear inverse problem is to determine \(x\in \mathbb{R}^n\) such that \(Cx \approx b\). Of course there need not be any vector \(x\) such that \(Cx = b\) (say if \(C\) is not invertible), and if there are multiple \(x\) that satisfy \(Cx \approx b\) it is not clear how we determine which is best. Furthermore, even if \(C\) were invertible, the problem may be ill-conditioned, in the sense that slight perturbations in the data \(C,b\) may cause the solution \(x\) to vary drastically. These situations are all undesirable, and we say the problem is <b>ill-posed</b>.</p> <p>A standard approach to determine a solution \(x\) is to frame this as an optimization problem:</p> \[\min_{x\in\mathbb{R}^d}\left\{ \frac{\alpha}{2}F(Cx,b) + R(x)\right\}\] <p>Here \(R\) is a <b>regularizer</b> intended to encourage solutions with desirable structure, and \(F(Cx,b)\) is a <b>fidelity term</b> that measures the difference between \(Cx\) and \(b\). It turns out that many popular choices of regularizer and fidelity term can be motivated from a statistical perspective. For example, the Tikhonov regularizer \(R = \|\cdot\|_2^2\) corresponds to performing maximum a posteriori (MAP) estimation with a Gaussian prior assumption on the data. Another example, the \(\ell_1\) regularizer \(R = \|\cdot\|_1\) corresponds to MAP estimation with a Laplacian prior assumption.</p> <p>It is natural to wonder if there is a meaningful way to choose your regularizer and fidelity terms based on knowledge of the given problem. One answer to this question comes from information theory. In 1957, E.T. Jaynes gave us the <a href="https://bayes.wustl.edu/etj/articles/theory.1.pdf" target="_blank" rel="noopener noreferrer">Principle of Maximum Entropy,</a> which roughly says that the probability distribution that best represents an unknown model is that with maximum entropy, subject to prior constraints. Taking this a step further, the <b>Principle of Maximum Entropy on the Mean</b> says that the state best describing an unknown system is the <b>mean</b> of the aforementioned maximum entropy distribution.</p> <p>We can translate these ideas to mathematics by adopting the Kullback-Leibler (KL) divergence as a measure of ‚Äúdistance‚Äù between probability distributions:</p> \[D_{KL}(Q||P) = \begin{cases} \int_\Omega \log\left(\frac{\mathrm{d}Q}{\mathrm{d}P}\right)\mathrm{d}Q, &amp; Q\ll P\\ +\infty, &amp; \text{otherwise} \end{cases}\] <p>The expression being integrated is the logarithm of the Radon-Nikodym derivative (when it exists). Now given a probability distribution \(P\) the <b>Maximum Entropy on the Mean (MEM) Function</b> <a href="https://journal.geophysicsjournal.com/JofG/article/download/87/48" target="_blank" rel="noopener noreferrer">[Rietsch, 1977]</a> \(\kappa_P \colon \mathbb{R}^d \to(-\infty,+\infty]\) is defined by</p> \[\kappa_P(y) = \inf\left\{D_{KL}(Q||P) \mid Q\ll P \text{ with } \mathbb{E}_Q = y\right\}\] <p>Despite all this talk of maximizing entropy, we observe that the MEM function is defined by a minimization problem. It can be shown that maximizing entropy is equivalent to minimizing distance from the uniform distribution, so here we are minimizing distance from the prior distribution \(P\) in order to maximize entropy while respecting the prior constraints. Intuitively, the MEM function gives us a way to numerically quantify the compliance of \(y\) with our prior distribution \(P\) by finding the closest distance (in the sense of KL) to a distribution \(Q \ll P\) with mean \(y\). In general the optimization problem defining \(\kappa_P(y)\) is an infinite-dimensional problem over a space of probability measures, so this representation is computationally intractable. We will deal with this shortly.</p> <p>To be concrete, we can consider reformulating the least squares inverse problem in terms of the MEM paradigm as follows. The solution \(\bar{x}\) is taken as the mean of the distribution minimizing the KL-regularized problem:</p> \[\bar{x} = \mathbb{E}_{\bar{Q}}[X], \bar{Q} \in \mathrm{argmin}_{Q\in \mathcal{P}(\Omega)} \left\{D_{KL}(Q||P) + \frac{\alpha}{2}\|b-C\mathbb{E}_Q[X]\|_2^2\right\}\] <p>We can instead package away all the infinite-dimensionality into the MEM function and write the equivalent problem:</p> \[\bar{x} = \mathrm{argmin}_{y\in \mathbb{R}^d}\left\{\frac{\alpha}{2}\|Cy-b\|_2^2 + \kappa_P(y)\right\}\] <p>This representation is not of much use unless we understand \(\kappa_P\). Fortunately, our main theoretical result establishes verifiable conditions under which \(\kappa_P\) admits an alternative finite-dimensional characterization as the convex conjugate of a familiar function. To be precise:</p> <p><u>Theorem 1 [Vaisbourd et al.]:</u> Suppose \(P \in \mathcal{P}(\Omega)\) generates a minimal and steep exponential family, and that one of the following holds:</p> <ul> <li>\(\Omega_P \text{ (the support of \(P\)) is uncountable}\)</li> <li>\(\Omega_P \text{ is countable and } \mathrm{conv}\Omega_P \text{ is closed}\)</li> </ul> <p>Then \(\kappa_P(y) = \psi_P^*(y) := \sup\{\langle y,\theta - \psi_P(\theta)\}\) where \(\psi_P(\theta) = \log \int_\Omega \exp(\langle y,\theta \rangle) dP(y)\) is the log-moment-generating-function for \(P\).</p> <p>The minimal and steep exponential family assumption deserves some more attention. The <b>standard exponential family generated by </b> \(P\) is the set of densities</p> \[\mathcal{F}_P := \{ f_{P_\theta}(y) := \exp(\langle y,\theta \rangle - \psi_P(\theta)) \mid \psi_P(\theta) &lt; \infty \}\] <p>Minimality is a mild topological assumptions on the domain of \(\psi_P\) and steepness is an assumption on the convexity and smoothness of \(\psi_P\). Under these assumptions we have a homeomorphic relationship between the domains of \(\psi_P\) and \(\psi_P^*\):</p> <div class="row justify-content-sm-center"> <img src="/assets/img/cramer.png" width="500" height="230"> </div> <div class="caption"> Illustration of the mean-value parametrization of an exponential family </div> <p>Now the bulk of the work is to compute these functions \(\psi_P^*\) (known as Cram√©r‚Äôs function from large deviations theory). Fortunately, we have derived closed-form expressions for the Cram√©r function for many popular probability distributions, as well as their (Bregman) proximal operators for solving the regularized problems algorithmically. For more information, check out our <a href="https://arxiv.org/abs/2211.05205" target="_blank" rel="noopener noreferrer">recent preprint,</a> soon to be paired with a Python package allowing users to experiment with solving custom regularized problems in the MEM framework.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2024 Ariel Goodwin. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>