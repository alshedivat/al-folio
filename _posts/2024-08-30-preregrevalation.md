---
layout: post
title: I did not want to write this blog post. 
date:   2023-10-25
description: The Preregistration Revelation
tags: openscience preregistration metascience
categories: [metascience, statistics, retraction preregistration]
---

# The Preregistration Revelation 

I did not want to write this blog post. This is why  I emailed the authors in November of last year, shared very detailed concerns and offered to walk them through them so they could retract on their own terms.

I did not want to write this blog post at any point in time over th past ten months while this winded through a long and thorough investigation. I did not want to write it when it was retracted. I don't like deep dive sleuthing from my own read of messy information. I want things that are concerning to be adjudicated by experts in the field and handled formally and appropriately. Especially issues this serious. I *did not want to write this post* I


All along, I had hoped the authors mistakes and move on. held out some shred of belief that people who have built careers on honesty, transparency and openness could be honest, open and transparent. I was naive. 

 I still don't want to write this post. But here it is... because the authors are unwilling to accurately represent the facts and that reflects poorly on my co-author, the investigating scientists, the hard-working team at NHB, and everyone who spent countless hours trying to figure out what should be done with this paper. 

I did not have time to write this post. I have a 9-5, am writing a book, and trying to be a scientist in what little time I have left. There will be typos.  If you find them, tell me. There may be errors, but this is from my notes and I have received nothing from the authors or during the investigation to correct any of the inferences drawn from their historical documents here. If you want to verify anything, go to their OSF. Download their historical files [especially this zip](https://osf.io/rnvxk) and read through draft and code history. It will take all of the hours spent by myself, the editors, and those reviewing the concerns that the authors are choosing to undermine. 

*Without further ado*

# Calling Bullshit
If you read the [retraction note](https://www.nature.com/articles/s41562-024-01997-3), the editors do not mince words about why this study was retracted

 1. "lack of transparency and misstatement of the hypotheses and predictions the reported meta-study was designed to test"
 2. "lack of preregistration for measures and analyses supporting the titular claim (against statements asserting preregistration in the published article)"
 3. "selection of outcome measures and analyses with knowledge of the data"
 4. "incomplete reporting of data and analyses."

The first is **Outcome Switching**. The authors switched the outcomes from an null finding to one that reflected their long standing advocacy work and incentives. 

The second is **fabrication and falsification**. They fabricated several false statements throughout the piece about the nature of their methods in conducting the analysis. If it were once, it might be an accident but it was repeatedly and they're continuing to to do it. 

The third is **HARKing**. They analyzed their data, noticed this compelling result and wrote the paper around it. They conducted statistical tests they are still claiming were preregistered (see below) with full knowledge of the data. They hacked and evolved metrics to align with their claims. 

The final issue is **selectively reporting** of analyses and data. Specifically, refusing to be open, honest and transparent about the nature of their pilots even though these issues were raised nearly four years ago by Tal Yarkoni in the original review. 

**The paper was rejected because the authors were found to have engaged extensively in one what their previous work has defined as questionable research practices**. Of course questionable research practices are *questionable* but when a group of experts on the subject engage in them repeatedly to push a claim it makes you start to question. 

# Feeling Not so HOT (Honest, Open and Transparent)
The reason I'm writing this damn blog post after putting it off for a year is precisely because of this post and Brian Nosek, the document it contains, and the myths it is spurring about the reasons for retraction. 

![](/assets/img/nosek.png){: width="750" }

In this post and in the [OSF File](https://osf.io/4k5sf), Brian seems to suggest that the problems mostly surrounded a single line in the paper. He provides no direct link to the retraction notice, does not engage with the content of the retration notice and does not engage with the specific issues and framing in our MA which describes the reasons why the paper was retracted. Instead, he focuses on something we mention only in passing, the inaccuracy of this line: 

>"All confirmatory tests, replications and analyses were preregistered both in the individual studies (Supplementary Information section 3 and Supplementary Table 2) and for this meta-project (https://osf.io/6t9vm)."


Read this in contrast with the journal's exact language for why the retraction occurred. The journal makes it abundantly clear the issues ran much deeper than a procedural error in a single sentence describing the preregistration and a few failures to disclose deviations. That would have been solvable with a correction and would not have taken ten months and countless hours, with extensive review by experts and in the field and at NPG. I understand that he disagrees with the journal's reason for retracting but that is no excuse for failing to engage substantively with the journal's rationale and create the impression the cause was quite different. 

The authors post-game write-up, along with describing their ability to submit a fresh manuscript to fresh peer review as an R&R has been very effective spin. Commentators on twitter appear to think this just isn't a big deal. 

![](/assets/img/spin.jpeg){: width="750" }


What is striking is that even now the authors don't seem to be able to get their story straight on what they preregistered. Later in [the note](https://osf.io/2s94g/), they explain how this specific language came about and highlight that elsewhere they've described things accurately:

![](/assets/img/manyconfirmatory.png){: width="750" }

This is bullshit. Let's take the very first analysis in that confirmatory analysis section.  It describes a multi-level meta-analysis that was coded up by James Pustejovsky: 

![](/assets/img/confirmatoryanalysis.png){: width="750" }

You can even check out the [pre-registered analysis code his described early on as exploratory](https://osf.io/938rv/). Notably, this is code they just opted not to make publicly available when they published the paper originally and it only came out during the investigation in response to *my noting it had not been shared*. This code just so happens to accurately reflect the truly preregistered analyses and their purpose--something the paper seemed incapable of doing. 

You know how we really know this meta-analysis is exploratory though? Because [an old version](https://osf.io/rnvxk) of the paper clearly indicate they added this analysis in March of 2020 ("Deciphering the Decline Effect P6_JEP.docx”). Well after Schooler [gave a talk](https://www.youtube.com/watch?v=TUiUA5O0PFk) on a preliminary analysis of the data. Conducting an analysis after having analyzed the data and observed pattern is, by the authors own definitions, [not a confirmatory test](https://www.pnas.org/doi/10.1073/pnas.1708274114). It's a QRP to present it as one. 

![](/assets/img/pustejovsky.png){: width="750" }

Hell, later in their correction they (maybe?) allude to this by saying that "some sensitivity and heterogeneity analyses" were not preregistered. But that doesn't fix the earlier sentence which misleadingly implies that elsewhere they got the description of what was preregistered right. Although, the supplement confusingly has a section describing the pre-registered analyses called "pre-registered analyses" within the confirmatory analyses. This was a source of hope for me throughout the process as it seemed much more like incompetence than malice. 

It's worth highlighting that this meta-analysis is different from their analysis of variation in replicability across labs across labs, which their correction notice indicates was *also* exploratory and relabeled as confirmatory... 

To really drive home the point that it wasn't just the sentence Brian keeps hammering on about, check [their supplement](https://static-content.springer.com/esm/art%3A10.1038%2Fs41562-023-01749-9/MediaObjects/41562_2023_1749_MOESM1_ESM.pdf) where they once again list the confirmatory analyses. Here they affirmatively claim that replication rate and the exploratory-turned-confirmatory lab-specific variation were confirmatory. Why does this section differ from the main text? Who knows? Hard to keep your story straight about what you preregistered if you've abandoned your prereg long ago. 

If their description of what was preregistered is a mistake, it's one they made at least three times in one paper and supplement and are continuing to make in their response to the retraction. Really straining Hanlon's razor on this one. 

# It is not just about the preregistration 

Hopefully the above makes it dreadfully clear that either the authors are either unable or unwilling to be honest, open and transparent about what they preregistered, even now. As noted above, if the issue were simply that they whiffed the preregistration description---the journal would have said that. I would have thought it was funny and moved on long ago. It would not have taken ten months. Instead the issues were much more serious.  

## Outcome Switching and lying by omission. 
One of the key reasons this paper was retracted is that the authors do not accurately describe their original, pre-registered motivation for the study. The reason it was funded. This is lying by omission and fabricating the motivation for the study. In combination with claims that this was preregistered, the authors are fabricating a story about how they arrived at these results that is simply untrue. 

Stephanie Lee's story covers the [supernatural hypothesis](https://www.chronicle.com/article/this-study-was-hailed-as-a-win-for-science-reform-now-its-being-retracted) Author Jonathan Schooler had long ago proposed that merely observing a phenomenon could change its effect size. Perhaps the other authors thought this was stupid, but that's a fantastic reason to either a) not be part of the project or b) make it clear your goals of joining the project in a preregistration before you conduct data analysis. The preregistrations occurred in 2018 and 2019, so the authors contention that 

The authors *were* somewhat transparent about their unconventional [supernatural explanation] in the early drafts of the paper as well. 

>“According to one theory of the decline effect, the decline is caused by a study being repeatedly run (i.e., an exposure effect)25. According to this account, the more studies run between the confirmation study and the self-replication, the greater the decline should be.”

This is nearly verbatim from the preregistration: 

>According to one theory of the decline effect, the decline is caused by a study being repeatedly run (i.e., an exposure effect). Thus, we predict that the more studies run between the confirmation study and the self-replication, the greater will be the decline effect.

It is also found in [responses to reviewers](https://osf.io/rnvxk) at Nature, who sensed the authors were testing a supernatural idea even though they had reframed things towards replication by this point: 

>“The short answer to the purpose of many of these features was to design the study a priori to address exotic possibilities for the decline effect that are at the fringes of scientific discourse….”

And in the [original title](https://osf.io/rnvxk): 

> The replicability of newly discovered psychological findings over repeated replications

It's also in their [original discussion](https://osf.io/rnvxk): 

>  Importantly, the number of times the initial investigators replicated an effect themselves is not predictive of the replicability of the effect by independent teams (Kunert, 2016). 

What's the one place it isn't clear what they set out to study? The published manuscript. The authors refer to these hypotheses that date back to the origin of the project as phenomena of secondary interest and do not describe the hypothesis explicitly. They refer only to this original motivation in the supplement of "test of unusual possible explanations"

![](/assets/img/secondaryinterest.png){: width="750" }


Why not? Well, the authors say so in their response to tal Yarkoni who asked them in review at Nature to come out and say it if they were testing a supernatural hypothesis. They decided to just change the message of the paper. 

>“As such we revised the manuscript so as not to dwell on the decline effect hypotheses and kept that discussion and motivation for the design elements to the SOM.”

When you read the paper, you'd have no idea this is what they got funding to study. Their omitted variables and undisclosed deviations in their maint-text statistical models make it harder to discern they were after the decline effect. These variables would have caused considerable head-scratching for someone reading the pre-registered analysis code which--fortunately for the authors---they forgot to make public when they published the manuscript. 

In distancing themselves for [two of the three reasons they got funding](https://www.fetzer-franklin-fund.org/projects/deciphering-the-decline-effect-1/) they mislead the reader about what they set out to study and why. This isn't a preregistration issue. This is outcome switching, and lying by omission.

The authors seem to indicate in their post-game anaysis that they had planned this framing and analysis all along. If so, it's damn surprising that none of this came up in the investigation. Its truly an oversight they forgot to include a link to documentation in their post-game write-up to something supporting that they had planned this all along. 

![](/assets/img/bestlaidplans.png){: width="750"}

Perhaps they're referring to their [operations manual](https://osf.io/ahdy7/files/osfstorage) but I certainly hope not. There is no plan to in that document that matches the description above. Here's there predictions, make of them what you will:

![](/assets/img/predictions.png){: width="750"}

If there is some time-stamped documentation that clearly states a desire to measure replicability as it occurs in the published article,  I cannot fathom why they did not produce it during the investigation or when I reached out in November. Perhaps it exists, but other evidence suggests otherwise. 

## HARKing, Hacking and Cherry-Picking
Failing to preregister descriptives used to make claims in the title is one thing, but the motivation for preregistration is specifically to prevent analyses from evolving after data has been observed in a way that provides stronger support for an argument. Here's their original write-up. 


Replicability was not found in the original preregistration. Nor is it found in the [analysis code dated 09/03/2019](https://osf.io/rnvxk). The first mention of estimating replicability was during Schooler’s talk on 09/05/2019, which estimated replicability as the proportion of significant results among the studies with significant confirmatory tests. 

![](/assets/img/schoolertalk.png){: width="750" }

Initially, they seemed to have been viewing the significant replications of null confirmatory findings as separate from their estimate of replicability. They were *surprised* by this result and Schooler frames it as an *incline effect*. 

Between Schooler's talk and the first draft of the write-up they wind up with [this abstract and general set of conclusions](https://osf.io/rnvxk) from their analysis: 

![](/assets/img/originalabstract.png){: width="750" }


This abstract for the earliest version of the manuscript states the replication rate is 92.2% (59/64) based on a definition that considers null replications of null confirmatory studies as “successful”. This is the opposite of the published definition which only considers significant replications of null confirmation studies as successful.  

![](/assets/img/originaldefinition.png){: width="750" }

There are a few things to note.  Their conclusion that "high-powered pre-registered studies can be effectively replicated by high-powered pre-registered replications". In the published version they've broaden to conclude that "This high replication rate justifies confidence in rigour-enhancing methods to increase the replicability of new discoveries." 

Their definition of replicability used in the final version is not referred to here as replicability but simply statistically significant results. *Their published estimate of replicability came together after seeing the data, generating a descriptive, and calling it something else*. But why switch to this definition? 

Their early replication rate is quite different and better even---92.2%. Yete it appears to be a mathematical error, as it includes 47 of the 52 replications of significant confirmatory studies and appears to include all 12 replications of null confirmatory studies regardless of outcome. When corrected, this definition would produce a replicability rate would be 79% (51/64). This would be considerably lower than this drafts chosen method of calculating statistical power (~100%), and lower than replicability in the published version (86%). It would also be lower than the highest estimates from the literature cited in the initial version and later omitted for reasons unstated (11/13 or 85% from Klein et al 2014) and undermine the key claims of the paper. They can't have that! 

It's tempting to think that perhaps their revised definition of replicability is reasonable and just fixing a math error. The trouble is that it's not a metric anyone would plan *a priori* because it is far too sensitive to the base rate. Imagine they had only found 8 "true" findings. 100% significant results in replication for these true findings and 100% replication nulls for the null confirmatory studies. Their measure would indicate 50% replicability---the same as they say is typical in the literature. 

As versions go on, they do a few things to bring their replicability above their power and the replicability in the literature. 

First, they revise their estimate of statistical power in a very strange way such that statistical power is zero when the sign of confirmatory test differs from predicted value even though the tests are two-tailed and the theoretical minimum is .05. They lump in the near 1.0 statistical power for significant confirmatory studies with this weird, at times zero, measure for null studies and goose statistical power down below their absurd and clearly opportunistic replicability estimate. 

Next, then adopt what was the initially referred to as the rate of significance for replications as the rate of replicability---a metric that as described above no one who spent a moments though on it would plan *a priori*. Certainly not a team of experts on replication. Taken together, their claim that their rate of replication exceeds the theoretical maximum only arises as a result of jointly evolving their estimate of power and replicability. This is Hacking statistics to make a claim. 

Finally, they even alter what they define as the literature-wide rate of replicability they compare against. A key claim is that they observed high replicability and the literature has not. In the main text cite a range of replicability (30-70%) which they seem to average at 50%. You might think the approximately 50% here is just the halfway point between 30 and 70. Nope, the talk by [Schooler] lays out their original math.

![](/assets/img/fortyseven.png){: width="750" }

There are plenty of problems with this, not the least of which is that several of these involve non-random sampling frames and the estimate of replicability is a result of the chosen mix of well-replicated and novel findings. But the real rub is Many Labs 1. 10 out of 13 is 77%. In reality it's [11 out of 13](https://econtent.hogrefe.com/doi/10.1027/1864-9335/a000178) or 85% using their criteria of significance. 

From the initial analysis to subsequent versions, they cite ML2 and ML3 but all of a sudden leave out this study which would have undermined that claim they were higher than the disappointing range in the literature. Their 92.2% might have looked better, but the 86% is right there with it. They also opt to leave out [Soto 2019](https://journals.sagepub.com/doi/abs/10.1177/0956797619831612) which has 87% successful replicability. It's hard to make a claim that rigor-enhancing techniques are necessary for replicability when two other studies have replicated portions of the literature at similar rates which most assuredly did not embrace these techniques in discovery. Perhaps they didn't know Soto's work, but they certainly knew Many Labs 1. The two projects shared authors. If there was some justified reason for selectively excluding mid write-up the one study in the literature that makes it impossible for them to make the claim... that would be a stroke of luck. This is a very weird QRP in parallel to selective removing of outliers. Maybe they'd argue it's not a QRP because it isn't a formal statistical test, but that didn't stop them from anchoring the titular claim on this comparison. 

So they cherry-picked their point of comparison, selectively omitted two conflicting points of comparison from their original framing, and hacked their estimates of power and replicability to be near one another. We really don't have to speculate that they cherry-picked. They come out and say the quiet part loud in an [earlier draft](https://osf.io/rnvxk). Here, their 75% confidence interval is lower than their their other metrics and they suggest that because it *should* have been high it should no longer be used as a metric. 

![](/assets/img/lowest.png){: width="750" }


## Omission

Frankly, all of the above could probably be weaseled around. Is it really a formal analyses? These are just descriptives. Everyone engages in QRPs so what's the big deal here? Maybe the results are still good? Isn't this obviously true so who cares if the authors embraced all of those things the authors are arguing need to be eradicated for science to work... 

Here's the *absolutely essential* problem. They are contending that they discovered a set of highly replicable findings when using rigor-enhancing best practices. Because the discoveries occurred during the piloting phase, this can *only* be logically true if they followed the practices in piloting. Because they committed to doing every hypothesis selected from piloting, the replicability of this body of literature was decided in piloting. 

You can read more about the problem in depth as written [by Hullman](http://users.eecs.northwestern.edu/~jhullman/Hullman_Protzko_et_al_comments.pdf), who reviewed the Matters arising and the concerns. Tal Yarkoni, at Nature, pointed out the need to be transparent about the piloting process 4 years ago. As you read the paper, ask yourself the following questions: 

- Did they verify all pilots were pre-registered and conducted without deviations?
- How many hypotheses were tested per pilot? 
- Were they allowed to enter exploratory, post-hoc analyses based on trends in the pilot? 
- What were the actual sample sizes of the pilots? 
- Etc... 

We get a hint from a post of Nosek's that rigor-enhancing best practices were not strictly required during the pilot. We also get an indication that effect sizes declined where we'd expect them to and the authors really didn't feel the need to mention this in the write-up. 

![](/assets/img/nosexplore.png){: width="750" }



Think about this for a second. If they found 16 hypotheses dredging out significant findings from n=200 experiments, committed to replicating them, and got 80% replicabilty or whatever... it implies that best practices are very much not needed to find replicable results. This is a *Huge* question on which the entire paper depends. Tal Pointed it out in his review. James did bring up this selection problem as early as comments on drafts of the [original preregistration in ~2018](https://osf.io/rnvxk). Why ignore your hired statistican, reviewers, and the concerns that might retract your paper and *still* refuse to click "make public" on those pilot repositories? Their draft suggests they're analyzing the data so why are they so reluctant to make them openly available to address the concerns myself, the editors, and Hullman had. 

A key reason for retraction is that these issues need to be addressed to support the claims the authors have not been forthcoming about this data. The authors just refused to provide open, honest, transparent descriptions of the piloting process as it occurred. Instead, they vaguely stated " All pilots were required to have their materials, procedure, hypotheses, analysis plan and exclusions preregistered prior to data collection. Ostensible discoveries were then nominated for confirmation and replication by the discovering labs."

It's odd they don't seem to note whether the *whole* pilot had to be nominated or single hypotheses from many outcomes could be selected. What few pilots I've found by weird google searches suggested they tested a lot of things and picked one of the hypotheses to pull forward. It's likewise odd that Brian's post seems to suggest considerably  more analytic flexibility than the text implies. 

Against this backdrop, it seems pretty damn misleading that their post-mortem simply says: 

> Also, our findings spurred interest in what occurred in the pilot phase of the project. We are likewise quite interested to learn more about the pilot phase.

Really? These concerns date back to their hired statistial consultant six years ago, were reiterated by a reviewer in 2020, brought up during the investigation by myself and a reviewer and they *still* haven't made the existing repositories that are sufficiently organized to permit ongoing analysis public. 

It is very difficult to square this with openness, honesty and transparency. 

# Putting it all together
As I said at the outset. I had no desire to write this blog post. I didn't want to wade into all of this. Their OSF is such a mess that getting claims straight and conveying them to others is a nightmare. Many of the links above will just open a massive zip file you'll have to navigate to find the corresponding file. Good luck, go with god.  Other parts of their repo including pilots  of are unlinked and can only be found by searching on the internet archive, finding a registration and working your way back up---often to a still private repository. And despite the fact that embargos for many pilots should have lifted if they were actually preregistered, I can't find most by searching in this manner. I have little doubt some will remain unconvinced because this is tl;dr and the files are td;dr (too disorganized, didn't read). 

But I had to write this blog post. The authors are being dishonest about the nature of the concerns, the nature of the process, and the results of the investigation.  They are under precisely no obligation to believe them, but absolutely are under an obligation to present their case honestly, openly and transparently without misleading the reader into believing a single sentence was the key motivation for retraction. In doing so they've expended a years worth of charitably assuming they're acting in good faith, honestly and transparently.

As outlined above, they the reason for retraction was very much more than the preregistration issues. It was a pattern of misleading statements throughout the piece that obscured a research process so rife with QRPs the results could not be trusted. It was refusal to make public key data that were necessary to validate the claims. I assure you I have only scratched the surface of the issues in this too-long-but-its-time-for-work blog post. 

Their utterly misleading description of why the paper was preregistered undermines the results of a 10 month investigation involving plenty of journal staff and four experts in the field--none of whom were myself or Berna. The reason we went that route--in lieu of detailing concerns from the beginning is because *I really did not want to write this post*. 