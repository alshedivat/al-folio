<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- ======================================================================= -->
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<style type="text/css">
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight:300;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }

  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
</style>
<!-- ======================================================================= -->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-114291442-6"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-114291442-6');
</script>


<script type="text/javascript" src="resources/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link rel="icon" type="image/png" href="../img/favicon.ico">
  <title>Navigating to Objects in the Real World</title>
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="https://www.youtube.com/iframe_api"></script>
</head>

<body>
      <br>
      <center><span style="font-size:44px;font-weight:bold;">Navigating to Objects in the Real World</span></center><br/>
      <table align=center width=600px>
      <tr>
        <td align=center width=100px>
        <center><span style="font-size:22px"><a href="https://theophilegervet.github.io/" target="_blank">Theophile Gervet</a></span></center></td>

        <td align=center width=100px>
        <center><span style="font-size:22px"><a href="https://soumith.ch/" target="_blank">Soumith Chintala</a></span></center></td>
          
        <td align=center width=100px>
        <center><span style="font-size:22px"><a href="https://faculty.cc.gatech.edu/~dbatra/" target="_blank">Dhruv Batra</a></span></center></td>

        <td align=center width=100px>
        <center><span style="font-size:22px"><a href="https://people.eecs.berkeley.edu/~malik/" target="_blank">Jitendra Malik</a></span></center></td>
          
        <td align=center width=160px>
        <center><span style="font-size:22px"><a href="https://devendrachaplot.github.io/" target="_blank">Devendra Singh Chaplot</a></span></center></td>
          
      <tr/>
      <tr>
        <td align=center width=100px>
        <center><span style="font-size:20px">CMU</span></center></td>
        <td align=center width=100px>
        <center><span style="font-size:20px">FAIR</span></center></td>
        <td align=center width=100px>
        <center><span style="font-size:20px">FAIR</span></center></td>
        <td align=center width=100px>
        <center><span style="font-size:20px">FAIR</span></center></td>
        <td align=center width=100px>
          <center><span style="font-size:20px">FAIR</span></center></td>
      <tr/>
      </table>
      <!-- <table align=center width=700px> -->
          <!-- <tr> -->
            <!-- <td align=center width=700px><center><span style="font-size:22px">Published at <a href="https://neurips.cc/">NeurIPS, 2021</a> </span></center></td> -->
          <!-- <tr/> -->
      <!-- </table><br/> -->
      <table align=center width=700px>
          <tr>
            <td align=center width=100px><center><span style="font-size:28px"><a href="https://arxiv.org/abs/2212.00922">[Arxiv]</a></span></center></td>
            <td align=center width=100px><center><span style="font-size:28px"><a href="https://www.science.org/doi/10.1126/scirobotics.adf6991">[Science Robotics]</a></span></center></td>
            <td align=center width=100px><center><span style="font-size:28px"><a href="https://github.com/facebookresearch/home-robot/">[Code]</a></span></center></td>
            <td align=center width=100px><center><span style="font-size:28px"><a href="https://www.youtube.com/embed/zZ6nlkTZVds">[Talk]</a></span></center></td>
            <td align=center width=100px><center><span style="font-size:28px"><a href="../../slides/real-world-object-navigation.pdf">[Slides]</a></span></center></td>
<!--            <td align=center width=100px><center><span style="font-size:28px"><a href='https://github.com/devendrachaplot/Neural-SLAM'>[GitHub Code]</a></span></center></td>-->
          <tr/>
      </table>
      
      </table>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1000px>
          <center><a href="../../assets/real-world-object-navigation/summary.gif"><img src = "../../assets/real-world-object-navigation/summary.gif" width="800px"></img></a><br></center>
        </td></tr>
      </table><br>

      <!--
      <div style="width:800px; margin:0 auto; text-align:justify">
        Semantic navigation is necessary to deploy mobile robots in uncontrolled environments like our homes, schools, and hospitals. 
        Many learning-based approaches have been proposed in response to the lack of semantic understanding of the classical pipeline for spatial navigation, which builds a geometric map using depth sensors and plans to reach point goals. 
        Broadly, end-to-end learning approaches reactively map sensor inputs to actions with deep neural networks, while modular learning approaches enrich the classical pipeline with learning-based semantic sensing and exploration. 
        While the field has no shortage of proposed methods, learned navigation policies have predominantly been evaluated in simulation, and it is unclear whether simulation is useful as an evaluation benchmark. 
        We address this issue through a large-scale empirical evaluation comparing representative methods from classical, modular learning, and end-to-end learning approaches across six visually diverse homes. 
        We find that modular learning transfers well, rising from a 81% simulation to a 90% real-world success rate. 
        In contrast, end-to-end learning does not, dropping from 77% simulation to 23% real-world success rate due to a large image domain gap between simulation and reality. 
        For practitioners, we show that modular learning is a reliable approach to navigate to objects. 
        For researchers, we identify two key issues that prevent today's simulators from being reliable evaluation benchmarks: (A) a large Sim-to-Real gap in images, which causes design choices to easily overfit to simulation, and (B) a disconnect between simulation and real-world error modes, which limits the usefulness of simulation to diagnose bottlenecks and further improve methods.
      </div>
      <br><hr>
      -->
      <div style="width:800px; margin:0 auto; text-align:justify">
        Semantic navigation is necessary to deploy mobile robots in uncontrolled environments like our homes, schools, and hospitals. 
        Many learning-based approaches have been proposed in response to the lack of semantic understanding of the classical pipeline for spatial navigation.
        But learned visual navigation policies have predominantly been evaluated in simulation. 
        How well do different classes of methods work on a robot? 
        We present a large-scale empirical study of semantic visual navigation methods comparing representative methods from classical, modular, and end-to-end learning approaches across six homes with no prior experience, maps, or instrumentation.
        We find that modular learning works well in the real world, attaining a 90% success rate. 
        In contrast, end-to-end learning does not, dropping from 77% simulation to 23% real-world success rate due to a large image domain gap between simulation and reality. 
        For practitioners, we show that modular learning is a reliable approach to navigate to objects: modularity and abstraction in policy design enable Sim-to-Real transfer.
        For researchers, we identify two key issues that prevent today's simulators from being reliable evaluation benchmarks — (A) a large Sim-to-Real gap in images and (B) a disconnect between simulation and real-world error modes — and propose concrete steps forward.
      </div>
      <br><hr>

      <center><h1>Object Goal Navigation</h1></center>
      <div style="width:800px; margin:0 auto; text-align:justify">
        We instantiate semantic navigation with the Object Goal navigation task, where a robot starts in a completely unseen environment and is asked to find an instance of an object category, let's say a toilet.
        The robot has access to only a first-person RGB and depth camera and a pose sensor.
      </div>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1200px>
          <center><a href="../../assets/real-world-object-navigation/objectnav_problem.png"><img src = "../../assets/real-world-object-navigation/objectnav_problem.png" width="800px"></img></a></center>
        </td></tr>
      </table>
      <br>

      <div style="width:800px; margin:0 auto; text-align:justify">
        This task is challenging. 
        The robot requires not only spatial scene understanding of distinguishing free space and obstacles and semantic scene understanding of detecting objects, but also requires learning semantic exploration priors. 
        For example, if a human wants to find a toilet in this scene, most of us would choose the hallway because it is most likely to lead to a toilet. 
        Teaching this kind of common sense or semantic priors to an autonomous agent is challenging.
        While exploring the scene for the desired object, the robot also needs a long-term episodic memory to remember explored and unexplored areas.
      </div>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1200px>
          <center><a href="../../assets/real-world-object-navigation/challenging_problem.png"><img src = "../../assets/real-world-object-navigation/challenging_problem.png" width="800px"></img></a><br></center>
        </td></tr>
      </table>
      <br><hr>

      <center><h1>Methods</h1></center>
      <div style="width:800px; margin:0 auto; text-align:justify">
      So how do we train autonomous agents capable of efficient navigation while tackling all these challenges?
      A classical approach to this problem builds a geometric map using depth sensors, explores the environment with a heuristic, like frontier exploration, which explores the closest unexplored region, and uses an analytical planner to reach exploration goals and the goal object as soon as it is in sight.
      An end-to-end learning approach predicts actions directly from raw observations with a deep neural network consisting of visual encoders for image frames followed by a recurrent layer for memory.
      A modular learning approach builds a semantic map by projecting predicted semantic segmentation using depth, predicts an exploration goal with a goal-oriented semantic policy as a function of the semantic map and the goal object, and reaches it with a planner.
      </div>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1200px>
          <center><a href="../../assets/real-world-object-navigation/methods.gif"><img src = "../../assets/real-world-object-navigation/methods.gif" width="800px"></img></a><br></center>
        </td></tr>
      </table>
      <br><hr>

      <center><h1>Large-scale Real-world Empirical Evaluation</h1></center>
      <div style="width:800px; margin:0 auto; text-align:justify">
        While many approaches to navigate to objects have been proposed over the past few years, learned navigation policies have predominantly been evaluated in simulation, which opens the field to the risk of sim-only research that does not generalize to the real world.
        We address this issue through a large-scale empirical evaluation of representative classical, end-to-end learning, and modular learning approaches across 6 unseen homes and 6 goal object categories.
      </div>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1000px>
          <center><a href="../../assets/real-world-object-navigation/empirical_evaluation.gif"><img src = "../../assets/real-world-object-navigation/empirical_evaluation.gif" width="800px"></img></a><br></center>
        </td></tr>
      </table>
      <br><hr>

      <center><h1>Results</h1></center>
      <div style="width:800px; margin:0 auto; text-align:justify">
      We compare approaches in terms of success rate within a limited budget of 200 robot actions and Success weighted by Path Length (SPL), a measure of path efficiency.
      In simulation, all approaches perform comparably, at around 80% success rate.
      But in the real world, modular learning and classical approaches transfer really well, up from 81% to 90% and 78% to 80% success rates, respectively.
      While end-to-end learning fails to transfer, down from 77% to 23% success rate.
      </div>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1000px>
          <center><a href="../../assets/real-world-object-navigation/results_quantitative.png"><img src = "../../assets/real-world-object-navigation/results_quantitative.png" width="800px"></img></a><br></center>
        </td></tr>
      </table>
      <br>

      <div style="width:800px; margin:0 auto; text-align:justify">
        We illustrate these results qualitatively with one representative trajectory. 
        All approaches start in a bedroom and are tasked with finding a couch. 
        On the left, modular learning first successfully reaches the couch goal. 
        In the middle, end-to-end learning fails after colliding too many times. 
        On the right, the classical policy finally reaches the couch goal after a detour through the kitchen.
      </div>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1000px>
          <center><a href="../../assets/real-world-object-navigation/results_qualitative.gif"><img src = "../../assets/real-world-object-navigation/results_qualitative.gif" width="800px"></img></a><br></center>
        </td></tr>
      </table>

      <center><h2>Result 1: Modular Learning is Reliable</h2></center>
      <div style="width:800px; margin:0 auto; text-align:justify">
        We find that modular learning is very reliable on a robot, with a 90% success rate.
        Here, we can see it finds a plant in a first home efficiently, a chair in a second home, and a toilet in a third.
      </div>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1000px>
          <center><a href="../../assets/real-world-object-navigation/modular_reliability.gif"><img src = "../../assets/real-world-object-navigation/modular_reliability.gif" width="800px"></img></a><br></center>
        </td></tr>
      </table>

      <center><h2>Result 2: Modular Learning Explores more Efficiently than Classical</h2></center>
      <div style="width:800px; margin:0 auto; text-align:justify">
        Modular learning improves by 10% real-world success rate over the classical approach.
        On the left, the goal-oriented semantic exploration policy directly heads towards the bedroom and finds the bed in 98 steps with an SPL of 0.90. 
        On the right, because frontier exploration is agnostic to the bed goal, the policy makes detours through the kitchen and the entrance hallway before finally reaching the bed in 152 steps with an SPL of 0.52. 
        With a limited time budget, inefficient exploration can lead to failure.
      </div>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1000px>
          <center><a href="../../assets/real-world-object-navigation/modular_vs_classical.gif"><img src = "../../assets/real-world-object-navigation/modular_vs_classical.gif" width="800px"></img></a><br></center>
        </td></tr>
      </table>

      <center><h2>Result 3: End-to-end Learning Fails to Transfer</h2></center>
      <div style="width:800px; margin:0 auto; text-align:justify">
        While classical and modular learning approaches work well on a robot, end-to-end learning does not, at only 23% success rate.
        The policy collides often, revisits the same places, and even fails to stop in front of goal objects when they are in sight.
      </div>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1000px>
          <center><a href="../../assets/real-world-object-navigation/end_to_end_failures.gif"><img src = "../../assets/real-world-object-navigation/end_to_end_failures.gif" width="800px"></img></a><br></center>
        </td></tr>
      </table>
      <br><hr>

      <center><h1>Analysis</h1></center>

      <center><h2>Insight 1: Why does Modular Transfer while End-to-end does not?</h2></center>
      <div style="width:800px; margin:0 auto; text-align:justify">
        Why does modular learning transfer so well while end-to-end learning does not? 
        To answer this question, we reconstructed one real-world home in simulation and conducted experiments with identical episodes in sim and reality.
      </div>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1000px>
          <center><a href="../../assets/real-world-object-navigation/reconstruction.gif"><img src = "../../assets/real-world-object-navigation/reconstruction.gif" width="800px"></img></a><br></center>
        </td></tr>
      </table>
      <br>

      <div style="width:800px; margin:0 auto; text-align:justify">
        The semantic exploration policy of the modular learning approach takes a semantic map as input, while the end-to-end policy directly operates on the RGB-D frames.
        The semantic map space is invariant between sim and reality, while the image space exhibits a large domain gap. 
        In this example, this gap leads to a segmentation model trained on real-world images to predict a bed false positive in the kitchen.
      </div>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1000px>
          <center><a href="../../assets/real-world-object-navigation/sim_vs_real_episode.gif"><img src = "../../assets/real-world-object-navigation/sim_vs_real_episode.gif" width="800px"></img></a><br></center>
        </td></tr>
      </table>
      <br>

      <div style="width:800px; margin:0 auto; text-align:justify">
        The semantic map domain invariance allows the modular learning approach to transfer well from sim to reality.
        In contrast, the image domain gap causes a large drop in performance when transferring a segmentation model trained in the real world to simulation and vice versa. 
        If semantic segmentation transfers poorly from sim to reality, it is reasonable to expect an end-to-end semantic navigation policy trained on sim images to transfer poorly to real-world images.
      </div>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1000px>
          <center><a href="../../assets/real-world-object-navigation/gaps_and_invariances.png"><img src = "../../assets/real-world-object-navigation/gaps_and_invariances.png" width="800px"></img></a><br></center>
        </td></tr>
      </table>

      <center><h2>Insight 2: Sim vs Real Gap in Error Modes for Modular Learning</h2></center>
      <div style="width:800px; margin:0 auto; text-align:justify">
        Surprisingly, modular learning works even better in reality than simulation. 
        Detailed analysis reveals that a lot of the failures of the modular learning policy that occur in sim are due to reconstruction errors, which do not happen in reality.
        Visual reconstruction errors represent 10% out of the total 19% episode failures, and physical reconstruction errors another 5%.        
        In contrast, failures in the real world are predominantly due to depth sensor errors, while most semantic navigation benchmarks in simulation assume perfect depth sensing. 
        Besides explaining the performance gap between sim and reality for modular learning, this gap in error modes is concerning because it limits the usefulness of simulation to diagnose bottlenecks and further improve policies.
        We show representative examples of each error mode and propose concrete steps forward to close this gap in the paper.
      </div>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1000px>
          <center><a href="../../assets/real-world-object-navigation/error_modes.png"><img src = "../../assets/real-world-object-navigation/error_modes.png" width="800px"></img></a><br></center>
        </td></tr>
      </table>
      <br><hr>

      <center><h1>Takeaways</h1></center>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1000px>
          <center><a href="../../assets/real-world-object-navigation/takeaways.png"><img src = "../../assets/real-world-object-navigation/takeaways.png" width="800px"></img></a><br></center>
        </td></tr>
      </table>

      <br><hr>

      <center><h1>Short Presentation</h1></center>
      <table align=center width=300px>
      <tr><td align=center width=300px>
        <iframe width="800" height="450" src="https://www.youtube.com/embed/zZ6nlkTZVds" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </td></tr>
      </table>
      <br>

      <div style="width:800px; margin:0 auto; text-align:justify">
        Voice by <a href="https://www.btrabucco.com/" target="_blank">Brandon Trabucco</a>.
      </div>


      <!--
      <br><hr>
      <table align=center width=850px>
        <center><h1>Paper and Bibtex</h1></center>
        <tr>
        <td width=200px align=left>
        <a href="../papers/neurips21_seal.pdf"><img style="width:200px" src="resources/thumbnail_nts.jpeg"/></a>
        <center>
        <span style="font-size:20pt"><a href="../papers/neurips21_seal.pdf">[Paper]</a>
        </center>
        </td>
        <td width=50px align=center>
        </td>
        <td width=550px align=left>
        <p style="text-align:left;"><b><span style="font-size:20pt">Citation</span></b><br/><span style="font-size:6px;">&nbsp;<br/></span> <span style="font-size:15pt">Chaplot, D.S., Dalal, M., Gupta, S., Malik, J. and Salakhutdinov, R. 2021. SEAL: Self-supervised Embodied Active Learning. In NeurIPS.</span></p>
        <span style="font-size:20pt"><a shape="rect" href="javascript:togglebib('assemblies19_bib')" class="togglebib">[Bibtex]</a></span>
        <div class="paper" id="assemblies19_bib">
                <pre xml:space="preserve">
    @inproceedings{chaplot2021seal,
    title={SEAL: Self-supervised Embodied Active Learning},
    author={Chaplot, Devendra Singh and Dalal, Murtaza, and Gupta, Saurabh 
            and Malik, Jitendra and Salakhutdinov, Ruslan},
    booktitle={NeurIPS},
    year={2021}}
                    </pre>
            </div>
            </td>
            </tr>
            <tr>
            <td width=250px align=left>
            </td>
            <td width=50px align=center>
            </td>
            <td width=550px align=left>
            
            </td>
            </tr>
        </table>
        <br><hr>
        -->

    <!--
    <center><h1>Related Projects</h1></center>
      <table align=center width=900px>
      <tr>
          <td align=center width=300px>
              <a href="./neural-slam.html">Active Neural SLAM</a>
          <center><a href="./neural-slam.html"><img src = "../img/pubs/ans.gif" height="120px"></img></a><br></center>
          </td>
          <td align=center width=300px>
              <a href="./semantic-exploration.html">Semantic Exploration</a>
          <center><a href="./semantic-exploration.html"><img src = "../img/pubs/semexp.gif" height="120px"></img></a><br></center>
          </td>
          <td align=center width=300px>
              <a href="./SemanticCuriosity.html">Semantic Curiosity</a>
          <center><a href="./SemanticCuriosity.html"><img src = "../img/pubs/semcur.gif" height="120px"></img></a><br></center>
          </td>
      </tr>
      </table>
    <br><hr>
    -->

    <!--
    <table align=center width=800px>
      <tr><td width=800px><left>
      <center><h1>Acknowledgements</h1></center>
      Carnegie Mellon University effort was supported in part by the US Army Grant W911NF1920104 and DSTA. UIUC effort is partially funded by NASA Grant 80NSSC21K1030. Jitendra Malik received funding support from ONR MURI (N00014-14-1-0671). Ruslan Salakhutdinov would also like to acknowledge NVIDIA’s GPU support.       
    <br>
          Website template from <a href="https://richzhang.github.io/colorization">here</a> and <a href="https://pathak22.github.io/modular-assemblies/">here</a>. <br>
      </left></td></tr>
    </table>
    -->

  <br><br>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
</body>
</html>