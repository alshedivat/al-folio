---
---

@article{ponton2022mmvr,
	bibtex_show={true},
	author = {Ponton, Jose Luis and Yun, Haoran and Andujar, Carlos and Pelechano, Nuria},
    title = {{Combining Motion Matching and Orientation Prediction to Animate Avatars for Consumer-Grade VR Devices}},
	booktitle = {ACM SIGGRAPH / Eurographics Symposium on Computer Animation},
    journal = {Computer Graphics Forum},
	publisher = {The Eurographics Association and John Wiley & Sons Ltd.},
	ISSN = {1467-8659},
	DOI = {10.1111/cgf.14628},
	year  = {2022},
	month = {September},
	preview = {mmvr.gif},
	selected = {true},
	code={https://github.com/UPC-ViRVIG/MMVR},
	website={https://upc-virvig.github.io/MMVR/},
	pdf={mmvr_sca2022.pdf},
	preview={mmvr.gif},
	abstract={The animation of user avatars plays a crucial role in conveying their pose, gestures, and relative distances to virtual objects
				or other users. Consumer-grade VR devices typically include three trackers: the Head Mounted Display (HMD) and
				two handheld VR controllers. Since the problem of reconstructing the user pose from such sparse data is ill-defined,
				especially for the lower body, the approach adopted by most VR games consists of assuming the body orientation matches
				that of the HMD, and applying animation blending and time-warping from a reduced set of animations. Unfortunately, this
				approach produces noticeable mismatches between user and avatar movements. In this work we present a new approach to
				animate user avatars for current mainstream VR devices. First, we use a neural network to estimate the user’s
				body orientation based on the tracking information from the HMD and the hand controllers. Then we use this orientation
				together with the velocity and rotation of the HMD to build a feature vector that feeds a Motion Matching algorithm. We built a
				MoCap database with animations of VR users wearing a HMD and used it to test our approach on both self-avatars and other
				users’ avatars. Our results show that our system can provide a large variety of lower body animations while correctly matching
				the user orientation, which in turn allows us to represent not only forward movements but also stepping in any direction.}
}

@inproceedings {ponton2022avatargo,
  bibtex_show={true},
  booktitle = {Eurographics 2022 - Short Papers},
  title = {{AvatarGo: Plug and Play self-avatars for VR}},
  author = {Ponton, Jose Luis and Monclus, Eva and Pelechano, Nuria},
  month = {May},
  year = {2022},
  publisher = {The Eurographics Association},
  ISSN = {1017-4656},
  ISBN = {978-3-03868-169-4},
  doi = {10.2312/egs.20221037},
  preview={avatargo.gif},
  selected={true},
  code={https://github.com/UPC-ViRVIG/AvatarGo},
  pdf={avatarGo_shortEG2022.pdf},
  abstract={The use of self-avatars in a VR application can enhance presence and embodiment which leads to a better user experience. In collaborative VR it also facilitates non-verbal communication. Currently it is possible to track a few body parts with cheap trackers and then apply IK methods to animate a character. However, the correspondence between trackers and avatar joints is typically fixed ad-hoc, which is enough to animate the avatar, but causes noticeable mismatches between the user's body pose and the avatar. In this paper we present a fast and easy to set up system to compute exact offset values, unique for each user, which leads to improvements in avatar movement. Our user study shows that the Sense of Embodiment increased significantly when using exact offsets as opposed to fixed ones. We also allowed the users to see a semitransparent avatar overlaid with their real body to objectively evaluate the quality of the avatar movement with our technique.}
}
