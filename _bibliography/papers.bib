---
---

@article{ponton2023sparseposer,
bibtex_show = {true},
selected = {true},
preview = {sparseposer.gif},
pdf = {sparseposer_siggraphasia2023.pdf},
code = {https://github.com/UPC-ViRVIG/SparsePoser},
website = {https://upc-virvig.github.io/SparsePoser/},
author = {Ponton, Jose Luis and Yun, Haoran and Aristidou, Andreas and Andujar, Carlos and Pelechano, Nuria},
title = {SparsePoser: Real-Time Full-Body Motion Reconstruction from Sparse Data},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
booktitle = {SIGGRAPH Asia 2023},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0730-0301},
url = {https://doi.org/10.1145/3625264},
doi = {10.1145/3625264},
abstract = {Accurate and reliable human motion reconstruction is crucial for creating natural interactions of full-body avatars in Virtual Reality (VR) and entertainment applications. As the Metaverse and social applications gain popularity, users are seeking cost-effective solutions to create full-body animations that are comparable in quality to those produced by commercial motion capture systems. In order to provide affordable solutions though, it is important to minimize the number of sensors attached to the subject’s body. Unfortunately, reconstructing the full-body pose from sparse data is a heavily under-determined problem. Some studies that use IMU sensors face challenges in reconstructing the pose due to positional drift and ambiguity of the poses. In recent years, some mainstream VR systems have released 6-degree-of-freedom (6-DoF) tracking devices providing positional and rotational information. Nevertheless, most solutions for reconstructing full-body poses rely on traditional inverse kinematics (IK) solutions, which often produce non-continuous and unnatural poses. In this article, we introduce SparsePoser, a novel deep learning-based solution for reconstructing a full-body pose from a reduced set of six tracking devices. Our system incorporates a convolutional-based autoencoder that synthesizes high-quality continuous human poses by learning the human motion manifold from motion capture data. Then, we employ a learned IK component, made of multiple lightweight feed-forward neural networks, to adjust the hands and feet toward the corresponding trackers. We extensively evaluate our method on publicly available motion capture datasets and with real-time live demos. We show that our method outperforms state-of-the-art techniques using IMU sensors or 6-DoF tracking devices, and can be used for users with different body dimensions and proportions.},
journal = {ACM Trans. Graph.},
month = {oct},
articleno = {5},
numpages = {14}
}


@article{ponton2023fittedavatars,
  bibtex_show = {true},
  title = {Fitted avatars: automatic skeleton adjustment for self-avatars in virtual reality},
  author = {Ponton, Jose Luis and Ceballos, Victor and Acosta, Lesly and Rios, Alejandro and Monclus, Eva and Pelechano, Nuria},
  journal = {Virtual Reality},
  pages = {2541--2560},
  issn = {1434-9957},
  month = jul,
  year = {2023},
  volume = {27},
  publisher = {Springer},
  selected = {true},
  preview = {fittedavatars.gif},
  pdf = {fittedavatars_vr2023.pdf},
  DOI = {10.1007/s10055-023-00821-z},
  abstract = {In the era of the metaverse, self-avatars are gaining popularity, as they can enhance presence and provide embodiment when a user is immersed in Virtual Reality. They are also very important in collaborative Virtual Reality to improve communication through gestures. Whether we are using a complex motion capture solution or a few trackers with inverse kinematics (IK), it is essential to have a good match in size between the avatar and the user, as otherwise mismatches in self-avatar posture could be noticeable for the user. To achieve such a correct match in dimensions, a manual process is often required, with the need for a second person to take measurements of body limbs and introduce them into the system. This process can be time-consuming, and prone to errors. In this paper, we propose an automatic measuring method that simply requires the user to do a small set of exercises while wearing a Head-Mounted Display (HMD), two hand controllers, and three trackers. Our work provides an affordable and quick method to automatically extract user measurements and adjust the virtual humanoid skeleton to the exact dimensions. Our results show that our method can reduce the misalignment produced by the IK system when compared to other solutions that simply apply a uniform scaling to an avatar based on the height of the HMD, and make assumptions about the locations of joints with respect to the trackers.}
}

@inproceedings{yun2023animationfidelity,
	bibtex_show = {true},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	title = {{Animation Fidelity in Self-Avatars: Impact on User Performance and Sense of Agency}},
	author = {Yun, Haoran and Ponton, Jose Luis and Andujar, Carlos and Pelechano, Nuria},
	year = {2023},
	month = {March},
	publisher = {{IEEE}},
	pages = {286-296},
	ISSN = {2642-5254},
	preview = {animationfidelity.gif},
	selected = {true},
	pdf = {animation_fidelity_ieeevr23.pdf},
	DOI = {10.1109/VR55154.2023.00044},
	abstract = {In this paper, we study the impact of the avatar's animation fidelity on different tasks. We compare three animation techniques: two of them using Inverse Kinematics to reconstruct the pose from six trackers, and a third one using a motion capture system with 17 inertial sensors. Our results show that the animation quality affects the Sense of Embodiment. Inertial-based MoCap performs significantly better in mimicking body poses. Surprisingly, IK-based solutions using fewer sensors outperformed MoCap in tasks requiring accurate positioning, which we attribute to the higher latency and the positional drift of the end-effectors.}
}
	
	
@article{ponton2022mmvr,
	bibtex_show={true},
	author = {Ponton, Jose Luis and Yun, Haoran and Andujar, Carlos and Pelechano, Nuria},
    title = {{Combining Motion Matching and Orientation Prediction to Animate Avatars for Consumer-Grade VR Devices}},
	booktitle = {ACM SIGGRAPH / Eurographics Symposium on Computer Animation},
    journal = {Computer Graphics Forum},
	volume = {41},
	number = {8},
	pages = {107-118},
	ISSN = {1467-8659},
	DOI = {10.1111/cgf.14628},
	year  = {2022},
	month = {September},
	preview = {mmvr.gif},
	selected = {true},
	code = {https://github.com/UPC-ViRVIG/MMVR},
	website = {https://upc-virvig.github.io/MMVR/},
	pdf = {mmvr_sca2022.pdf},
	abstract = {The animation of user avatars plays a crucial role in conveying their pose, gestures, and relative distances to virtual objects
				or other users. Consumer-grade VR devices typically include three trackers: the Head Mounted Display (HMD) and
				two handheld VR controllers. Since the problem of reconstructing the user pose from such sparse data is ill-defined,
				especially for the lower body, the approach adopted by most VR games consists of assuming the body orientation matches
				that of the HMD, and applying animation blending and time-warping from a reduced set of animations. Unfortunately, this
				approach produces noticeable mismatches between user and avatar movements. In this work we present a new approach to
				animate user avatars for current mainstream VR devices. First, we use a neural network to estimate the user’s
				body orientation based on the tracking information from the HMD and the hand controllers. Then we use this orientation
				together with the velocity and rotation of the HMD to build a feature vector that feeds a Motion Matching algorithm. We built a
				MoCap database with animations of VR users wearing a HMD and used it to test our approach on both self-avatars and other
				users’ avatars. Our results show that our system can provide a large variety of lower body animations while correctly matching
				the user orientation, which in turn allows us to represent not only forward movements but also stepping in any direction.}
}

@inproceedings {ponton2022avatargo,
  bibtex_show={true},
  booktitle = {Eurographics 2022 - Short Papers},
  title = {{AvatarGo: Plug and Play self-avatars for VR}},
  author = {Ponton, Jose Luis and Monclus, Eva and Pelechano, Nuria},
  month = {May},
  year = {2022},
  publisher = {The Eurographics Association},
  ISSN = {1017-4656},
  ISBN = {978-3-03868-169-4},
  doi = {10.2312/egs.20221037},
  preview = {avatargo.gif},
  selected = {true},
  code = {https://github.com/UPC-ViRVIG/AvatarGo},
  pdf = {avatarGo_shortEG2022.pdf},
  abstract={The use of self-avatars in a VR application can enhance presence and embodiment which leads to a better user experience. In collaborative VR it also facilitates non-verbal communication. Currently it is possible to track a few body parts with cheap trackers and then apply IK methods to animate a character. However, the correspondence between trackers and avatar joints is typically fixed ad-hoc, which is enough to animate the avatar, but causes noticeable mismatches between the user's body pose and the avatar. In this paper we present a fast and easy to set up system to compute exact offset values, unique for each user, which leads to improvements in avatar movement. Our user study shows that the Sense of Embodiment increased significantly when using exact offsets as opposed to fixed ones. We also allowed the users to see a semitransparent avatar overlaid with their real body to objectively evaluate the quality of the avatar movement with our technique.}
}
