@article{2014bachelorthesis,
  title={GPU-accelerated stochastic simulator engine for PRISM model checker.},
  author={Marcin Copik},
  year={2014},
  pdf = {2014_prism_gpu_bachelor_thesis.pdf},
  slides = {2014_prism_gpu_bachelor_thesis_slides.pdf},
  journal = {Bachelor Thesis},
  abstract = {This project provides a new simulator engine for the PRISM model checker, an enhancement allowing for faster approximate model checking. The simulator is designed as a substitute for the current engine for simple integration with GUI and CLI. The engine was implemented with the OpenCL, an open standard for massively parallel computing on heterogeneous platforms. The engine generates a proper OpenCL kernel for a PRISM model, which will execute on OpenCL devices. This approach enables the generation of samples both on CPU and GPU. The performance and correctness tests included three case studies taken from the official PRISM benchmark. The results showed a huge gain in performance over the existing simulator; in the most extreme case, the new engine, working on seven years old NVIDIA GPU, verified a test property in 20 seconds, where the existing simulator engine needed over two hours.},
  abbr = {Thesis}
}

@Article{doi:10.3109/10929088.2014.891657,
  author    = {Dominik Spinczyk and Adam Karwan and Marcin Copik},
  journal   = {Computer Aided Surgery},
  title     = {Methods for abdominal respiratory motion tracking},
  year      = {2014},
  number    = {1-3},
  pages     = {34-47},
  volume    = {19},
  doi       = {10.3109/10929088.2014.891657},
  eprint    = {https://doi.org/10.3109/10929088.2014.891657},
  publisher = {Taylor & Francis},
  url       = {https://doi.org/10.3109/10929088.2014.891657},
  abstract = {Non-invasive surface registration methods have been developed to register and track breathing motions in a patient's abdomen and thorax. We evaluated several different registration methods, including marker tracking using a stereo camera, chessboard image projection, and abdominal point clouds. Our point cloud approach was based on a time-of-flight (ToF) sensor that tracked the abdominal surface. We tested different respiratory phases using additional markers as landmarks for the extension of the non-rigid Iterative Closest Point (ICP) algorithm to improve the matching of irregular meshes. Four variants for retrieving the correspondence data were implemented and compared. Our evaluation involved 9 healthy individuals (3 females and 6 males) with point clouds captured in opposite breathing phases (i.e., inhalation and exhalation). We measured three factors: surface distance, correspondence distance, and marker error. To evaluate different methods for computing the correspondence measurements, we defined the number of correspondences for every target point and the average correspondence assignment error of the points nearest the markers.},
  html = {https://doi.org/10.3109/10929088.2014.891657},
  pdf = {2014_respiratory_motion_cas_paper.pdf},
  bibtex_show = {True},
  abbr = {CAS}
}

@inproceedings{2016gpu,
  author    = {Marcin Copik and
            Artur Rataj and
                           Bozena Wozna{-}Szczesniak},
  editor    = {Bernd{-}Holger Schlingloff},
  title     = {A GPGPU-based Simulator for Prism: Statistical Verification of Results
              of {PMC} (extended abstract)},
  booktitle = {Proceedings of the 25th International Workshop on Concurrency, Specification
            and Programming, Rostock, Germany, September 28-30, 2016},
  series    = {{CEUR} Workshop Proceedings},
  volume    = {1698},
  pages     = {199--208},
  publisher = {CEUR-WS.org},
  year      = {2016},
  url       = {http://ceur-ws.org/Vol-1698/CS\&P2016\_19\_Copik\&Rataj\&Wozna-Szczesniak\_A-GPGPU-based-Simulator-for-Prism-Statistical-Verification-of-Results-of-PMC.pdf},
  timestamp = {Wed, 12 Feb 2020 16:45:14 +0100},
  biburl    = {https://dblp.org/rec/conf/csp/CopikRW16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  code = {https://github.com/mcopik/prism-fastsim},
  html = {http://ceur-ws.org/Vol-1698/},
  pdf = {2016_prism_gpu_csp_paper.pdf},
  slides = {2016_prism_gpu_csp_slides.pdf},
  abstract = {We describe a GPGPU–based Monte Carlo simulator integrated with Prism. It supports Markov chains with discrete or continuous time and a subset of properties expressible in PCTL, CSL and their variants extended with rewards. The simulator allows an automated statistical verification of results obtained using Prism’s formal methods.},
  bibtex_show = {True},
  abbr = {CS&P}
}

@inproceedings{2017sycl,
  author = {Copik, Marcin and Kaiser, Hartmut},
  title = {Using SYCL as an Implementation Framework for HPX.Compute},
  year = {2017},
  isbn = {9781450352147},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3078155.3078187},
  doi = {10.1145/3078155.3078187},
  abstract = {The recent advancements in High Performance Computing and ongoing research to reach Exascale has been heavily supported by introducing dedicated massively parallel accelerators. Programmers wishing to maximize utilization of current supercomputers are required to develop software which not only involves scaling across multiple nodes but are capable of offloading data-parallel computation to dedicated hardware such as graphic processors. Introduction of new types of hardware has been followed by developing new languages, extensions, compilers and libraries. Unfortunately, none of those solutions seem to be fully portable and independent from specific vendor and type of hardware.HPX.Compute, a programming model developed on top of HPX, a C++ standards library for concurrency and parallelism, uses existing and proposed C++ language and library capabilities to support various types of parallelism. It aims to provide a generic interface allowing for writing code which is portable between hardware architectures.We have implemented a new backend for HPX.Compute based on SYCL, a Khronos standard for single-source programming of OpenCL devices in C++. We present how this runtime may be used to target OpenCL devices through our C++ API. We have evaluated performance of new implementation on graphic processors with STREAM benchmark and compare results with existing CUDA-based implementation.},
  booktitle = {Proceedings of the 5th International Workshop on OpenCL},
  articleno = {30},
  numpages = {7},
  keywords = {GPGPU, parallel programming, SYCL, C++, heterogeneous programming, HPX},
  location = {Toronto, Canada},
  series = {IWOCL 2017},
  slides = {2017_hpx_compute_iwocl_slides.pdf},
  html = {https://dl.acm.org/doi/10.1145/3078155.3078187?cid=99659165688},
  pdf = {2017_hpx_compute_iwocl_paper.pdf},
  bibtex_show = {True},
  abbr = {IWOCL}
}

@inproceedings{10.1145/3168804,
  author = {Barthels, Henrik and Copik, Marcin and Bientinesi, Paolo},
  title = {The Generalized Matrix Chain Algorithm},
  year = {2018},
  isbn = {9781450356176},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3168804},
  doi = {10.1145/3168804},
  abstract = {In this paper, we present a generalized version of the matrix chain algorithm to generate efficient code for linear algebra problems, a task for which human experts often invest days or even weeks of works. The standard matrix chain problem consists in finding the parenthesization of a matrix product M := A1 A2 ⋯ An that minimizes the number of scalar operations. In practical applications, however, one frequently encounters more complicated expressions, involving transposition, inversion, and matrix properties. Indeed, the computation of such expressions relies on a set of computational kernels that offer functionality well beyond the simple matrix product. The challenge then shifts from finding an optimal parenthesization to finding an optimal mapping of the input expression to the available kernels. Furthermore, it is often the case that a solution based on the minimization of scalar operations does not result in the optimal solution in terms of execution time. In our experiments, the generated code outperforms other libraries and languages on average by a factor of about 9. The motivation for this work comes from the fact that—despite great advances in the development of compilers—the task of mapping linear algebra problems to optimized kernels is still to be done manually. In order to relieve the user from this complex task, new techniques for the compilation of linear algebra expressions have to be developed.},
  booktitle = {Proceedings of the 2018 International Symposium on Code Generation and Optimization},
  pages = {138–148},
  numpages = {11},
  keywords = {matrix chain problem, linear algebra, compiler},
  location = {Vienna, Austria},
  series = {CGO 2018},
  arxiv = {1804.04021},
  html = {https://dl.acm.org/doi/10.1145/3168804?cid=99659165688},
  pdf = {2018_matrix_chain_cgo_paper.pdf},
  bibtex_show = {True},
  abbr = {CGO}
}

@InProceedings{10.1007/978-3-030-47956-5_15,
  author="Calotoiu, Alexandru
  and Copik, Marcin
  and Hoefler, Torsten
  and Ritter, Marcus
  and Shudler, Sergei
  and Wolf, Felix",
  editor="Bungartz, Hans-Joachim
  and Reiz, Severin
  and Uekermann, Benjamin
  and Neumann, Philipp
  and Nagel, Wolfgang E.",
  title="ExtraPeak: Advanced Automatic Performance Modeling for HPC Applications",
  booktitle="Software for Exascale Computing - SPPEXA 2016-2019",
  year="2020",
  publisher="Springer International Publishing",
  address="Cham",
  pages="453--482",
  abstract="Performance models are powerful tools allowing developers to understand the behavior of their applications, and empower them to address performance issues already during the design or prototyping phase. Unfortunately, the difficulties of creating such models manually and the effort involved render performance modeling a topic limited to a relatively small community of experts. This article summarizes the results of the two projects Catwalk, which aimed to create tools that automate key activities of the performance modeling process, and ExtraPeak, which built upon the results of Catwalk and worked toward making this powerful methodology more flexible, streamlined and easy to use. The sew projects both provide accessible tools and methods that bring performance modeling to a wider audience of HPC application developers. Since its outcome represents the final state of the two projects, we expand to a greater extent on the results of ExtraPeak.",
  isbn="978-3-030-47956-5",
  html = {https://link.springer.com/chapter/10.1007/978-3-030-47956-5_15},
  pdf = {2020_extrapeak_paper.pdf},
  bibtex_show = {True},
  abbr = {Book Chapter}
}

@article{2017prefixsum,
  title={Parallel Prefix Algorithms for the Registration of Arbitrarily Long Electron Micrograph Series}, 
  author={Marcin Copik and Paolo Bientinesi and Benjamin Berkels},
  year={2017},
  pdf = {2017_prefix_sum_abstract_SC.pdf},
  poster = {2017_prefix_sum_poster_SC.pdf},
  abstract = {Recent advances in the technology of transmission electron microscopy have allowed for a more precise visualization of materials and physical processes, such as metal oxidation. Nevertheless, the quality of information is limited by the damage caused by an electron beam, movement of the specimen or other environmental factors. A novel registration method has been proposed to remove those limitations by acquiring a series of low dose microscopy frames and performing a computational registration on them to understand and visualize the sample. This process can be represented as a prefix sum with a complex and computationally intensive binary operator and a parallelization is necessary to enable processing long series of microscopy images. With our parallelization scheme, the time of registration of results from ten seconds of microscopy acquisition has been decreased from almost thirteen hours to less than seven minutes on 512 Intel IvyBridge cores.},
  html = {https://sc17.supercomputing.org/SC17%20Archive/src_poster/src_poster_pages/spost118.html},
  journal = {ACM Student Research Competition at ACM/IEEE Supercomputing},
  abbr = {ACM SRC}
}

@article{2017masterthesis,
  title={Parallel Prefix Algorithms for the Registration of Arbitrarily Long Electron Micrograph Series}, 
  author={Marcin Copik},
  year={2017},
  pdf = {2017_prefix_sum_thesis.pdf},
  journal = {Master Thesis},
  abstract = {Recent advances in the technology of transmission electron microscopy have allowed for a more precise visualization of materials and physical processes, such as metal oxidation. Nevertheless, the quality of information is limited by the damage caused by an electron beam, movement of the specimen or other environmental factors. A novel registration method has been proposed to remove those limitations by acquiring a series of low dose microscopy frames and performing a computational registration on them to understand and visualize the sample. This process can be represented as a prefix sum with a complex and computationally intensive binary operator and a parallelization is necessary to enable processing long series of microscopy images. With our parallelization scheme, the time of registration of results from ten seconds of microscopy acquisition has been decreased from almost thirteen hours to less than seven minutes on 512 Intel IvyBridge cores.},
  arxiv = {1712.02533},
  bibtex_show = {True},
  abbr = {Thesis}
}

@periodical{2020prefixsum,
  title={Work-stealing prefix scan: Addressing load imbalance in large-scale image registration}, 
  author={Marcin Copik and Tobias Grosser and Torsten Hoefler and Paolo Bientinesi and Benjamin Berkels},
  year={2022},
  arxiv = {2010.12478},
  pdf = {2021_prefix_sum_TPDS.pdf},
  code = {https://github.com/berkels/match-series/},
  type = {article},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  html = {https://ieeexplore.ieee.org/document/9477174},
  url = {https://ieeexplore.ieee.org/document/9477174},
  abstract = {
    Parallelism patterns (e.g., map or reduce) have proven to be effective tools for parallelizing high-performance applications. In this paper, we study the recursive registration of a series of electron microscopy images - a time consuming and imbalanced computation necessary for nano-scale microscopy analysis. We show that by translating the image registration into a specific instance of the prefix scan, we can convert this seemingly sequential problem into a parallel computation that scales to over thousand of cores. We analyze a variety of scan algorithms that behave similarly for common low-compute operators and propose a novel work-stealing procedure for a hierarchical prefix scan. Our evaluation shows that by identifying a suitable and well-optimized prefix scan algorithm, we reduce time-to-solution on a series of 4,096 images spanning ten seconds of microscopy acquisition from over 10 hours to less than 3 minutes (using 1024 Intel Haswell cores), enabling derivation of material properties at nanoscale for long microscopy image series. 
  },
  volume={33},
  number={3},
  pages={523-535},
  doi={10.1109/TPDS.2021.3095230},
  bibtex_show = {True},
  abbr = {TPDS}
}

@inproceedings{copik2020sebs,
  title={SeBS: A Serverless Benchmark Suite for Function-as-a-Service Computing}, 
  author={Marcin Copik and Grzegorz Kwasniewski and Maciej Besta and Michal Podstawski and Torsten Hoefler},
  year={2021},
  arxiv={2012.14132},
  abstract = {Function-as-a-Service (FaaS) is one of the most promising directions for the future of cloud services, and serverless functions have immediately become a new middleware for building scalable and cost-efficient microservices and applications. However, the quickly moving technology hinders reproducibility, and the lack of a standardized benchmarking suite leads to ad-hoc solutions and microbenchmarks being used in serverless research, further complicating metaanalysis and comparison of research solutions. To address this challenge, we propose the Serverless Benchmark Suite: the first benchmark for FaaS computing that systematically covers a wide spectrum of cloud resources and applications. Our benchmark consists of the specification of representative workloads, the accompanying implementation and evaluation infrastructure, and the evaluation methodology that facilitates reproducibility and enables interpretability. We demonstrate that the abstract model of a FaaS execution environment ensures the applicability of our benchmark to multiple commercial providers such as AWS, Azure, and Google Cloud. Our work facilities experimental evaluation of serverless systems, and delivers a standardized, reliable and evolving evaluation methodology of performance, efficiency, scalability and reliability of middleware FaaS platforms.},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3464298.3476133},
  pages = {64–78},
  numpages = {15},
  keywords = {function-as-a-service, benchmark, serverless, FaaS},
  location = {Qu\'{e}bec city, Canada},
  booktitle = {Proceedings of the 22nd International Middleware Conference},
  series = {Middleware '21},
  code = {https://github.com/spcl/serverless-benchmarks},
  doi = {10.1145/3464298.3476133},
  artifact= {https://zenodo.org/record/5357597},
  pdf = {2021_sebs_middleware.pdf},
  slides = {2021_sebs_middleware_slides.pdf},
  html = {https://dl.acm.org/doi/10.1145/3464298.3476133?cid=99659165688},
  recording = {https://www.youtube.com/watch?v=Z_CQfh0pQjc},
  docker = {spcleth/serverless-benchmarks},
  bibtex_show = {True},
  abbr = {Middleware},
  selected = {true}
}

@article{2019perftaint,
  title={perf-taint: Taint Analysis for Automatic Many-Parameter Performance Modeling},
  author={Marcin Copik and Torsten Hoefler},
  year={2019},
  pdf = {2019_perf_taint_abstract.pdf},
  poster = {2019_perf_taint_poster.pdf},
  slides = {2019_perf_taint_slides.pdf},
  abstract = {Performance modeling is a well-known technique for understanding the scaling behavior of an application. Although the modeling process is today often automatic, it still relies on a domain expert selecting program parameters and deciding relevant sampling intervals. Since existing empirical methods attempt blackbox modeling, the decision on which parameters influence a selected part of the program is based on measured data, making empirical modeling sensitive to human errors and instrumentation noise. We introduce a hybrid analysis to mitigate the current limitations of empirical modeling, combining the confidence of static analysis with the ability of dynamic taint analysis to capture the effects of control-flow and memory operations. We construct models of computation and communication volumes that help the modeler to remove effects of noise and improve the correctness of estimated models. Our automatic analysis prunes irrelevant program parameters and brings an understanding of parameter dependencies which helps in designing the experiment.},
  html = {https://sc19.supercomputing.org/proceedings/src_poster/src_poster_pages/spostg110.html},
  journal = {ACM Student Research Competition at ACM/IEEE Supercomputing},
  note = {Gold Medal in the competition},
  abbr = {ACM SRC}
}

@inproceedings{2021perftaint,
  author = {Copik, Marcin and Calotoiu, Alexandru and Grosser, Tobias and Wicki, Nicolas and Wolf, Felix and Hoefler, Torsten},
  title = {Extracting Clean Performance Models from Tainted Programs},
  year = {2021},
  isbn = {9781450382946},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3437801.3441613},
  doi = {10.1145/3437801.3441613},
  abstract = {Performance models are well-known instruments to understand the scaling behavior of parallel applications. They express how performance changes as key execution parameters, such as the number of processes or the size of the input problem, vary. Besides reasoning about program behavior, such models can also be automatically derived from performance data. This is called empirical performance modeling. While this sounds simple at the first glance, this approach faces several serious interrelated challenges, including expensive performance measurements, inaccuracies inflicted by noisy benchmark data, and overall complex experiment design, starting with the selection of the right parameters. The more parameters one considers, the more experiments are needed and the stronger the impact of noise. In this paper, we show how taint analysis, a technique borrowed from the domain of computer security, can substantially improve the modeling process, lowering its cost, improving model quality, and help validate performance models and experimental setups.},
  booktitle = {Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages = {403–417},
  numpages = {15},
  keywords = {performance modeling, high-performance computing, compiler techniques, LLVM, taint analysis},
  location = {Virtual Event, Republic of Korea},
  series = {PPoPP '21},
  arxiv = {2012.15592},
  html = {https://dl.acm.org/doi/10.1145/3437801.3441613?cid=99659165688},
  pdf = {2021_perf_taint_ppopp.pdf},
  code = {https://github.com/spcl/perf-taint/},
  recording = {https://www.youtube.com/watch?v=eGEvFXK4owc},
  slides = {2021_perf_taint_ppopp_slides.pdf},
  artifact = {https://zenodo.org/record/4381803},
  docker = {spcleth/perf-taint},
  bibtex_show = {True},
  abbr = {PPoPP}
}

@inproceedings{besta2021graphminesuite,
  title={GraphMineSuite: Enabling High-Performance and Programmable Graph Mining Algorithms with Set Algebra}, 
  author={Maciej Besta and Zur Vonarburg-Shmaria and Yannick Schaffner and Leonardo Schwarz and Grzegorz Kwasniewski and Lukas Gianinazzi and Jakub Beranek and Kacper Janda and Tobias Holenstein and Sebastian Leisinger and Peter Tatkowski and Esref Ozdemir and Adrian Balla and Marcin Copik and Philipp Lindenberger and Pavel Kalvoda and Marek Konieczny and Onur Mutlu and Torsten Hoefler},
  year={2021},
  eprint={2103.03653},
  arxiv={2103.03653},
  archivePrefix={arXiv},
  primaryClass={cs.DC},
  abstract = {We propose GraphMineSuite (GMS): the first benchmarking suite for graph mining that facilitates evaluating and constructing high-performance graph mining algorithms. First, GMS comes with a benchmark specification based on extensive literature review, prescribing representative problems, algorithms, and datasets. Second, GMS offers a carefully designed software platform for seamless testing of different fine-grained elements of graph mining algorithms, such as graph representations or algorithm subroutines. The platform includes parallel implementations of more than 40 considered baselines, and it facilitates developing complex and fast mining algorithms. High modularity is possible by harnessing set algebra operations such as set intersection and difference, which enables breaking complex graph mining algorithms into simple building blocks that can be separately experimented with. GMS is supported with a broad concurrency analysis for portability in performance insights, and a novel performance metric to assess the throughput of graph mining algorithms, enabling more insightful evaluation. As use cases, we harness GMS to rapidly redesign and accelerate state-of-the-art baselines of core graph mining problems: degeneracy reordering (by up to >2x), maximal clique listing (by up to >9x), k-clique listing (by 1.1x), and subgraph isomorphism (by up to 2.5x), also obtaining better theoretical performance bounds.
  },
  month={08},
  booktitle={Proceedings of the 47th International Conference on Very Large Data Bases (VLDB'21)},
  html={http://vldb.org/pvldb/vol14-volume-info/},
  pdf={2021_gms_paper.pdf},
  artifact = {https://graphminesuite.spcl.inf.ethz.ch/},
  doi = {10.14778/3476249.3476252},
  bibtex_show = {True},
  abbr = {VLDB}
}

@inproceedings{besta2021sisa,
  title={SISA: Set-Centric Instruction Set Architecture for Graph Mining on Processing-in-Memory Systems}, 
  author={Maciej Besta and Raghavendra Kanakagiri and Grzegorz Kwasniewski and Rachata Ausavarungnirun and Jakub Beránek and Konstantinos Kanellopoulos and Kacper Janda and Zur Vonarburg-Shmaria and Lukas Gianinazzi and Ioana Stefan and Juan Gómez Luna and Marcin Copik and Lukas Kapp-Schwoerer and Salvatore Di Girolamo and Marek Konieczny and Onur Mutlu and Torsten Hoefler},
  year={2021},
  arxiv={2104.07582},
  abstract={Simple graph algorithms such as PageRank have recently been the target of numerous hardware accelerators. Yet, there also exist much more complex graph mining algorithms for problems such as clustering or maximal clique listing. These algorithms are memory-bound and thus could be accelerated by hardware techniques such as Processing-in-Memory (PIM). However, they also come with non-straightforward parallelism and complicated memory access patterns. In this work, we address this with a simple yet surprisingly powerful observation: operations on sets of vertices, such as intersection or union, form a large part of many complex graph mining algorithms, and can offer rich and simple parallelism at multiple levels. This observation drives our cross-layer design, in which we (1) expose set operations using a novel programming paradigm, (2) express and execute these operations efficiently with carefully designed set-centric ISA extensions called SISA, and (3) use PIM to accelerate SISA instructions. The key design idea is to alleviate the bandwidth needs of SISA instructions by mapping set operations to two types of PIM: in-DRAM bulk bitwise computing for bitvectors representing high-degree vertices, and near-memory logic layers for integer arrays representing low-degree vertices. Set-centric SISA-enhanced algorithms are efficient and outperform hand-tuned baselines, offering more than 10x speedup over the established Bron-Kerbosch algorithm for listing maximal cliques. We deliver more than 10 SISA set-centric algorithm formulations, illustrating SISA's wide applicability.},
  month={10},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  html = {https://doi.org/10.1145/3466752.3480133?cid=99659165688},
  url = {https://doi.org/10.1145/3466752.3480133},
  doi = {10.1145/3466752.3480133},
  booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  series = {MICRO '21},
  pdf={2021_micro_paper.pdf},
  artifact = {https://graphminesuite.spcl.inf.ethz.ch/},
  bibtex_show = {True},
  abbr = {MICRO}
}

@inproceedings{schmid2022perfdetective,
  author = {Schmid, Larissa and Copik, Marcin and Calotoiu, Alexandru and Werle, Dominik and Reiter, Andreas and Selzer, Michael and Koziolek, Anne and Hoefler, Torsten},
  title = {Performance-Detective: Automatic Deduction of Cheap and Accurate Performance Models},
  year = {2022},
  isbn = {9781450392815},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3524059.3532391},
  doi = {10.1145/3524059.3532391},
  abstract = {The many configuration options of modern applications make it difficult for users to select a performance-optimal configuration. Performance models help users in understanding system performance and choosing a fast configuration. Existing performance modeling approaches for applications and configurable systems either require a full-factorial experiment design or a sampling design based on heuristics. This results in high costs for achieving accurate models. Furthermore, they require repeated execution of experiments to account for measurement noise. We propose Performance-Detective, a novel code analysis tool that deduces insights on the interactions of program parameters. We use the insights to derive the smallest necessary experiment design and avoiding repetitions of measurements when possible, significantly lowering the cost of performance modeling. We evaluate Performance-Detective using two case studies where we reduce the number of measurements from up to 3125 to only 25, decreasing cost to only 2.9% of the previously needed core hours, while maintaining accuracy of the resulting model with 91.5% compared to 93.8% using all 3125 measurements.},
  booktitle = {Proceedings of the 36th ACM International Conference on Supercomputing},
  articleno = {3},
  numpages = {13},
  keywords = {empirical performance modeling, automatic performance modeling, configurable systems, experiment design},
  location = {Virtual Event},
  series = {ICS '22},
  pdf = {2022_ics_schmid_perf_detective.pdf},
  html = {https://doi.org/10.1145/3524059.3532391?cid=99659165688},
  artifact = {https://doi.org/10.5445/IR/1000146001},
  bibtex_show = {True},
  abbr = {ICS}
}

@inproceedings{copik2022faaskeeper,
  url = {https://arxiv.org/abs/2203.14859},
  author = {Copik, Marcin and Calotoiu, Alexandru and Zhou, Pengyu and Taranov, Konstantin and Hoefler, Torsten},
  title = {FaaSKeeper: Learning from Building Serverless Services with ZooKeeper as an Example},
  abstract = {FaaS (Function-as-a-Service) brought a fundamental shift into cloud computing: (persistent) virtual machines have been replaced with dynamically allocated resources, trading locality and statefulness for a pay-as-you-go model more suitable for varying and infrequent workloads. However, adapting services to functions in the serverless paradigm while still fulfilling functional requirements is challenging. In this work, we demonstrate how ZooKeeper, a centralized coordination service that offers a safe and wait-free consensus mechanism, can be redesigned to benefit from serverless computing. We define synchronization primitives to extend the capabilities of scalable cloud storage and contribute a set of requirements for efficient and scalable FaaS computing. We introduce FaaSKeeper, the first coordination service built on serverless functions and cloud-native services, and share serverless design lessons based on our experiences of implementing a ZooKeeper model deployable to clouds today. FaaSKeeper provides the same consistency guarantees and interface as ZooKeeper, with a serverless price model that lowers costs up to 450 times on infrequent workloads.},
  arxiv={2203.14859},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual, non-exclusive license},
  code={https://github.com/spcl/faaskeeper/},
  pdf = {2022_faaskeeper_preprint.pdf},
  bibtex_show = {True},
  abbr = {HPDC},
  series = {HPDC '24},
  selected = {true}
}

@inproceedings{copik2022fmi, 
  author = {Copik, Marcin and B\"{o}hringer, Roman and Calotoiu, Alexandru and Hoefler, Torsten},
  title = {FMI: Fast and Cheap Message Passing for Serverless Functions},
  year = {2023},
  isbn = {9798400700569},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3577193.3593718},
  doi = {10.1145/3577193.3593718},
  abstract = {Serverless functions provide elastic scaling and a fine-grained billing model, making Function-as-a-Service (FaaS) an attractive programming model. However, for distributed jobs that benefit from large-scale and dynamic parallelism, the lack of fast and cheap communication is a major limitation. Individual functions cannot communicate directly, group operations do not exist, and users resort to manual implementations of storage-based communication. This results in communication times multiple orders of magnitude slower than those found in HPC systems. We overcome this limitation and present the FaaS Message Interface (FMI). FMI is an easy-to-use, high-performance framework for general-purpose point-to-point and collective communication in FaaS applications. We support different communication channels and offer a model-driven channel selection according to performance and cost expectations. We model the interface after MPI and show that message passing can be integrated into serverless applications with minor changes, providing portable communication closer to that offered by high-performance systems. In our experiments, FMI can speed up communication for a distributed machine learning FaaS application by up to 162x, while simultaneously reducing cost by up to 397 times.},
  booktitle = {Proceedings of the 37th International Conference on Supercomputing},
  pages = {373–385},
  numpages = {13},
  keywords = {faas, function-as-a-service, serverless, I/O, high-performance computing},
  location = {Orlando, FL, USA},
  series = {ICS '23},
  arxiv={2305.08763},
  code={https://github.com/spcl/fmi/},
  bibtex_show = {True},
  abbr = {ICS},
  pdf = {2023_ics_fmi.pdf},
  html = {https://dl.acm.org/doi/10.1145/3577193.3593718?cid=99659165688},
  selected = {true}
}

@inproceedings{copik2024disagg,
  author = {Copik, Marcin and Chrapek, Marcin and Schmid, Larissa and Calotoiu, Alexandru and Hoefler, Torsten},
  title = {Software Resource Disaggregation for HPC with Serverless Computing},
  abstract = {Aggregated HPC resources have rigid allocation systems and programming models which struggle to adapt to diverse and changing workloads. Consequently, HPC systems fail to efficiently use the large pools of unused memory and increase the utilization of idle computing resources. Prior work attempted to increase the throughput and efficiency of supercomputing systems through workload co-location and resource disaggregation. However, these methods fall short of providing a solution that can be applied to existing systems without major hardware modifications and performance losses. In this paper, we improve the utilization of supercomputers by employing the new cloud paradigm of serverless computing. We show how serverless functions provide fine-grained access to the resources of batch-managed cluster nodes. We present an HPC-oriented Function-as-a-Service (FaaS) that satisfies the requirements of high-performance applications. We demonstrate a \emph{software resource disaggregation} approach where placing functions on unallocated and underutilized nodes allows idle cores and accelerators to be utilized while retaining near-native performance.},
  year = {2024},
  bibtex_show = {True},
  abbr = {IPDPS},
  series = {IPDPS '24},
  booktitle = {Proceedings of the 38th IEEE Interational Parallel and Distributed Processing Symposium},
  pdf = {2022_software_disaggregation_preprint.pdf},
  arxiv={2401.10852}
}

@inproceedings{2021rfaas,
  title={{r}FaaS: Enabling High Performance Serverless with RDMA and Leases},
  author={Marcin Copik and Konstantin Taranov and Alexandru Calotoiu and Torsten Hoefler},
  year={2023},
  arxiv={2106.13859},
  abstract={High performance is needed in many computing systems, from batch-managed supercomputers to general-purpose cloud platforms. However, scientific clusters lack elastic parallelism, while clouds cannot offer competitive costs for high-performance applications. In this work, we investigate how modern cloud programming paradigms can bring the elasticity needed to allocate idle resources, decreasing computation costs and improving overall data center efficiency. Function-as-a-Service (FaaS) brings the pay-as-you-go execution of stateless functions, but its performance characteristics cannot match coarse-grained cloud and cluster allocations. To make serverless computing viable for high-performance and latency-sensitive applications, we present rFaaS, an RDMA-accelerated FaaS platform. We identify critical limitations of serverless - centralized scheduling and inefficient network transport - and improve the FaaS architecture with allocation leases and microsecond invocations. We show that our remote functions add only negligible overhead on top of the fastest available networks, and we decrease the execution latency by orders of magnitude compared to contemporary FaaS systems. Furthermore, we demonstrate the performance of rFaaS by evaluating real-world FaaS benchmarks and parallel applications. Overall, our results show that new allocation policies and remote memory access help FaaS applications achieve high performance and bring serverless computing to HPC.
  },
  code={https://github.com/spcl/rFaaS/},
  bibtex_show = {True},
  abbr = {IPDPS},
  series = {IPDPS '23},
  booktitle = {Proceedings of the 37th IEEE Interational Parallel and Distributed Processing Symposium},
  artifact= {https://zenodo.org/record/7657524},
  pdf = {2023_ipdps_rfaas.pdf},
  selected = {true}
}

@misc{copik2022praas,
  author = {Copik, Marcin and Calotoiu, Alexandru and Bruno, Rodrigo and Rethy, Gyorgy and Böhringer, Roman and Hoefler, Torsten},
  title = {Process-as-a-Service: Elastic and Stateful Serverless with Cloud Processes},
  abstract = {
    Fine-grained and ephemeral functions power many new applications that benefit from elastic scaling and pay-as-you-use billing model with minimal infrastructure management overhead. To achieve these properties, Function-as-a-Service (FaaS) platforms disaggregate compute and state and, consequently, introduce non-trivial costs due to data locality loss, complex control plane interactions, and expensive communication to access state. We revisit the foundations of FaaS and propose a new cloud abstraction, cloud process, that retains all the benefits of FaaS while significantly reducing the overheads that result from the disaggregation. We show how established operating system abstractions can be adapted to provide powerful granular computing on dynamically provisioned cloud resources while building our Process as a Service (PraaS) platform. PraaS improves current FaaS by offering data locality, fast invocations, and efficient communication. PraaS delivers invocations up to 32 times faster and reduces communication overhead by up to 99%.
  },
  year = {2022},
  bibtex_show = {True},
  abbr = {arXiv},
  pdf = {2022_praas_preprint.pdf}
}

@article{copik2022disaggregationsrc,
  title={Software Resource Disaggregation for HPC with Serverless Computing},
  author={Marcin Copik and Alexandru Calotoiu and Torsten Hoefler},
  year={2022},
  pdf = {2022_softw_disagg_abstract.pdf},
  poster = {2022_softw_disagg_poster.pdf},
  slides = {2022_softw_disagg_slides.pdf},
  abstract = {Aggregated HPC resources have rigid allocation systems and programming models and struggle to adapt to diverse and changing workloads. Thus, HPC systems fail to efficiently use the large pools of unused memory and increase the utilization of idle computing resources. Prior work attempted to increase the throughput and efficiency of supercomputing systems through workload co-location and resource disaggregation. However, these methods fall short of providing a solution that can be applied to existing systems without major hardware modifications and performance losses. In this project, we use the new cloud paradigm of serverless computing to improve the utilization of supercomputers. We show that the FaaS programming model satisfies the requirements of high-performance applications and how idle memory helps resolve cold startup issues. We demonstrate a software resource disaggregation approach where the co-location of functions allows idle cores and accelerators to be utilized while retaining near-native performance.},
  html = {https://sc22.supercomputing.org/proceedings/src_poster/src_poster_pages/spostg103.html},
  journal = {ACM Student Research Competition at ACM/IEEE Supercomputing},
  note = {Gold Medal in the competition},
  abbr = {ACM SRC}
}

@article{chelini2022mom,
  title={MOM: Matrix Operations in MLIR},
  author={Chelini, Lorenzo and Barthels, Henrik and Bientinesi, Paolo and Copik, Marcin and Grosser, Tobias and Spampinato, Daniele G},
  year={2022},
  abstract = {Modern research in code generators for dense linear algebra computations has shown the ability to produce optimized code with a performance which compares and often exceeds the one of state-of-the-art implementations by domain experts. However, the underlying infrastructure is often developed in isolation making the interconnection of logically combinable systems complicated if not impossible. In this paper, we propose to leverage MLIR as a unifying compiler infrastructure for the optimization of dense linear algebra operations. We propose a new MLIR dialect for expressing linear algebraic computations including matrix properties to enable high-level algorithmic transformations. The integration of this new dialect in MLIR enables end-to-end compilation of matrix computations via conversion to existing lower-level dialects already provided by the framework.},
  url = {https://acohen.gitlabpages.inria.fr/impact/impact2022/papers/paper7.pdf},
  html = {https://acohen.gitlabpages.inria.fr/impact/impact2022/papers/paper7.pdf},
  journal = {12th International Workshop on Polyhedral Compilation Techniques},
  series = {IMPACT 2022},
  pdf = {2022_impact_chelini_mom.pdf},
  bibtex_show = {True},
  abbr = {IMPACT}
}

@article{2023ipdpsphdforum,
  title={High-Performance Serverless for HPC and Clouds},
  author={Marcin Copik and Torsten Hoefler},
  year={2023},
  pdf = {2023_ipdps_phd_forum_poster_summary.pdf},
  poster = {2023_ipdps_phd_forum_poster.pdf},
  abstract = {Function-as-a-Service (FaaS) computing brought a fundamental shift in resource management. It allowed for new and better solutions to the problem of low resource utilization, an issue that has been known for decades in data centers. The problem persists as the frequently changing resource availability cannot be addressed entirely with the techniques employed so far, such as persistent cloud allocations and batch jobs. The elastic fine-grained tasking and largely unconstrained scheduling of FaaS create new opportunities. Still, modern serverless platforms struggle to achieve the high performance needed for the most demanding and latency-critical workloads. Furthermore, many applications cannot be “FaaSified” without non-negligible loss in performance, and the short and stateless functions must be easy to program, debug, and optimize. By solving the fundamental performance challenges of FaaS, we can build a fast and efficient programming model that brings innovative cloud techniques into HPC data centers, allowing users to benefit from pay-as-you-go billing and helping operators to decrease running costs and their environmental impact. My PhD research attempts to bridge the gap between high-performance programming and modern FaaS computing frameworks. I have been working on tailored solutions for different levels of the FaaS computing stack: from computing and network devices to high-level optimizations and efficient system designs.},
  journal = {PhD Forum at 37th IEEE International Parallel & Distributed Processing Symposium (IPDPS)},
  abbr = {IPDPS}
}

@article{2023scdoctoral,
  title={High-Performance Serverless for HPC and Clouds},
  author={Marcin Copik and Torsten Hoefler},
  year={2023},
  poster = {2023_sc_doctoral_showcase_poster.pdf},
  abstract = {Function-as-a-Service (FaaS) computing brought a fundamental shift in resource management. It allowed for new and better solutions to the problem of low resource utilization, an issue that has been known for decades in data centers. The problem persists as the frequently changing resource availability cannot be addressed entirely with the techniques employed so far, such as persistent cloud allocations and batch jobs. The elastic fine-grained tasking and largely unconstrained scheduling of FaaS create new opportunities. Still, modern serverless platforms struggle to achieve the high performance needed for the most demanding and latency-critical workloads. Furthermore, many applications cannot be “FaaSified” without non-negligible loss in performance, and the short and stateless functions must be easy to program, debug, and optimize. By solving the fundamental performance challenges of FaaS, we can build a fast and efficient programming model that brings innovative cloud techniques into HPC data centers, allowing users to benefit from pay-as-you-go billing and helping operators to decrease running costs and their environmental impact. My PhD research attempts to bridge the gap between high-performance programming and modern FaaS computing frameworks. I have been working on tailored solutions for different levels of the FaaS computing stack: from computing and network devices to high-level optimizations and efficient system designs.},
  journal = {Doctoral Showcase at ACM/IEEE Supercomputing (S23)},
  abbr = {SC}
}

@article{2023upmbigdata, 
  author = {Wei Qiu and Marcin Copik and Yun Wang and Alexandru Calotoiu and Torsten Hoefler},
  title = {User-guided Page Merging for Memory Deduplication in Serverless Systems},
  year = {2023},
  booktitle = {2023 IEEE International Conference on Big Data (Big Data)},
  publisher = {IEEE Computer Society},
  pdf = {2023_bigdata_upm.pdf},
  slides = {2023_bigdata_upm_slides.pdf},
  abstract = {
    Serverless computing is an emerging cloud paradigm that offers an elastic and scalable allocation of computing resources with pay-as-you-go billing. In the Function-as-a-Service (FaaS) programming model, applications comprise short-lived and stateless serverless functions executed in isolated containers or microVMs, which can quickly scale to thousands of instances and process terabytes of data. This flexibility comes at the cost of duplicated runtimes, libraries, and user data spread across many function instances, and cloud providers do not utilize this redundancy. The memory footprint of serverless forces removing idle containers to make space for new ones, which decreases performance through more cold starts and fewer data caching opportunities. We address this issue by proposing deduplicating memory pages of serverless workers with identical content, based on the content-based page-sharing concept of Linux Kernel Same-page Merging (KSM). We replace the background memory scanning process of KSM, as it is too slow to locate sharing candidates in short-lived functions. Instead, we design User-Guided Page Merging (UPM), a built-in Linux kernel module that leverages the madvise system call: we enable users to advise the kernel of memory areas that can be shared with others. We show that UPM reduces memory consumption by up to 55% on 16 concurrent containers executing a typical image recognition function, more than doubling the density for containers of the same function that can run on a system.
  },
  abbr = {Big Data},
  bibtex_show = {True}
}

@article{2023cppless,
  title={Cppless: Productive and Performant Serverless Programming in C++},
  author={Lukas M\"oller and Marcin Copik and Alexandru Calotoiu and Torsten Hoefler},
  year={2023},
  pdf = {2023_cppless_paper.pdf},
  abstract = {The rise of serverless introduced a new class of scalable, elastic and highly
available parallel workers in the cloud. Many systems and applications benefit
from offloading computations and parallel tasks to dynamically allocated
resources. However, the developers of C++ applications found it difficult to
integrate functions due to complex deployment, lack of compatibility between
client and cloud environments, and loosely typed input and output data. To
enable single-source and efficient serverless acceleration in C++, we introduce
Cppless, an end-to-end framework for implementing serverless functions which
handles the creation, deployment, and invocation of functions. Cppless is built
on top of LLVM and requires only two compiler extensions to automatically
extract C++ function objects and deploy them to the cloud. We demonstrate that
offloading parallel computations from a C++ application to serverless workers
can provide up to 30x speedup, requiring only minor code modifications and
costing less than one cent per computation.
  },
  abbr = {arXiv},
  year = {2023},
  bibtex_show = {True}
}

@misc{2024xaas,
    title={XaaS: Acceleration as a Service to Enable Productive High-Performance Cloud Computing}, 
    author={Torsten Hoefler and Marcin Copik and Pete Beckman and Andrew Jones and Ian Foster and Manish Parashar and Daniel Reed and Matthias Troyer and Thomas Schulthess and Dan Ernst and Jack Dongarra},
    year={2024},
    eprint={2401.04552},
    archivePrefix={arXiv},
    primaryClass={cs.DC},
    abbr = {IEEE CiSE},
    arxiv={2106.13859},
    bibtex_show = {2401.04552},
    abstract = {HPC and Cloud have evolved independently, specializing their innovations into performance or productivity. Acceleration as a Service (XaaS) is a recipe to empower both fields with a shared execution platform that provides transparent access to computing resources, regardless of the underlying cloud or HPC service provider. Bridging HPC and cloud advancements, XaaS presents a unified architecture built on performance-portable containers. Our converged model concentrates on low-overhead, high-performance communication and computing, targeting resource-intensive workloads from climate simulations to machine learning. XaaS lifts the restricted allocation model of Function-as-a-Service (FaaS), allowing users to benefit from the flexibility and efficient resource utilization of serverless while supporting long-running and performance-sensitive workloads from HPC.}
}
