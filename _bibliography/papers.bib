---
---

@misc{learningCombinatorialOptimization2022,
  title = {Learning with {{Combinatorial Optimization Layers}}: A {{Probabilistic Approach}}},
  author = {Dalle, Guillaume and Baty, LÃ©o and Bouvier, Louis and Parmentier, Axel},
  date = {2022-07-27},
  number = {arXiv:2207.13513},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.13513},
  url = {https://arxiv.org/abs/2207.13513},
  arxiv = {2207.13513},
  abbr = {Preprint},
  abstract = {Combinatorial optimization (CO) layers in machine learning (ML) pipelines are a powerful tool to tackle data-driven decision tasks, but they come with two main challenges. First, the solution of a CO problem often behaves as a piecewise constant function of its objective parameters. Given that ML pipelines are typically trained using stochastic gradient descent, the absence of slope information is very detrimental. Second, standard ML losses do not work well in combinatorial settings. A growing body of research addresses these challenges through diverse methods. Unfortunately, the lack of well-maintained implementations slows down the adoption of CO layers. In this paper, building upon previous works, we introduce a probabilistic perspective on CO layers, which lends itself naturally to approximate differentiation and the construction of structured losses. We recover many approaches from the literature as special cases, and we also derive new ones. Based on this unifying perspective, we present InferOpt.jl, an open-source Julia package that 1) allows turning any CO oracle with a linear objective into a differentiable layer, and 2) defines adequate losses to train pipelines containing such layers. Our library works with arbitrary optimization algorithms, and it is fully compatible with Julia's ML ecosystem. We demonstrate its abilities using a pathfinding problem on video game maps.},
  bibtex_show = {true},
  code = {https://github.com/axelparmentier/InferOpt.jl},
  hal = {https://hal.archives-ouvertes.fr/hal-03739485},
  selected = {true}
}
