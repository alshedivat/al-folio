---
---
@article{fu2023learning,
  title={DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data},
  author={Stephanie Fu* and Netanel Tamir* and Shobhita Sundaram* and Lucy Chai and Richard Zhang and Tali Dekel and Phillip Isola},
  journal={arXiv:2306.09344},
  year={2023},
  preview={dreamsim_icon.png},
  paper = {https://arxiv.org/abs/2306.09344},
  website = {https://dreamsim-nights.github.io},
  abstract = {Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic content while also being sensitive to color and layout. Notably, despite being trained on synthetic data, our metric generalizes to real images, giving strong results on retrieval and reconstruction tasks. Furthermore, our metric outperforms both prior learned metrics and recent large vision models on these tasks.},
  selected=true
}

@article{hamilton2021axiomatic,
  title={Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning},
  author={Hamilton, Mark and Lundberg, Scott and Zhang, Lei and Fu, Stephanie and Freeman, William T},
  journal={International Conference on Learning Representations (ICLR)},
  year={2021},
  abbr={ICLR},
  preview={expl_icon.png},
  selected=true
}


@InProceedings{pmlr-v133-hamilton21a,
  title = 	 {MosAIc: Finding Artistic Connections across Culture with Conditional Image Retrieval},
  author =       {Hamilton, Mark and Fu, Stephanie and Lu, Mindren and Bui, Johnny and Bopp, Darius and Chen, Zhenbang and Tran, Felix and Wang, Margaret and Rogers, Marina and Zhang, Lei and Hoder, Chris and Freeman, William T.},
  booktitle = 	 {Proceedings of the NeurIPS 2020 Competition and Demonstration Track},
  pages = 	 {133--155},
  year = 	 {2021},
  editor = 	 {Escalante, Hugo Jair and Hofmann, Katja},
  volume = 	 {133},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--12 Dec},
  publisher =    {PMLR},
  paper = 	 {http://proceedings.mlr.press/v133/hamilton21a/hamilton21a.pdf},
  url = 	 {https://proceedings.mlr.press/v133/hamilton21a.html},
  blog={https://www.microsoft.com/en-us/garage/blog/2020/08/mit-students-build-mosaic-to-explore-art-across-cultures-at-microsoft-garage/},
  website={https://aka.ms/mosaic},
  abstract = 	 {We introduce MosAIc, an interactive web app that allows users to find pairs of semantically related artworks that span different cultures, media, and millennia. To create this application, we introduce Conditional Image Retrieval (CIR) which combines visual similarity search with user supplied filters or “conditions”. This technique allows one to find pairs of similar images that span distinct subsets of the image corpus. We provide a generic way to adapt existing image retrieval data-structures to this new domain and provide theoretical bounds on our approach’s efficiency. To quantify the performance of CIR systems, we introduce new datasets for evaluating CIR methods and show that CIR performs non-parametric style transfer. Finally, we demonstrate that our CIR data-structures can identify “blind spots” in Generative Adversarial Networks (GAN) where they fail to properly model the true data distribution.},
  abbr = {NeurIPS PMLR},
  preview = {mosaic_icon.png},
  selected=true
}

@article{Loke2021,
author={Loke, Gabriel
and Khudiyev, Tural
and Wang, Brian
and Fu, Stephanie
and Payra, Syamantak
and Shaoul, Yorai
and Fung, Johnny
and Chatziveroglou, Ioannis
and Chou, Pin-Wen
and Chinn, Itamar
and Yan, Wei
and Gitelson-Kahn, Anna
and Joannopoulos, John
and Fink, Yoel},
title={Digital electronics in fibres enable fabric-based machine-learning inference},
journal={Nature Communications},
year={2021},
month={Jun},
day={03},
volume={12},
number={1},
pages={3317},
blog={https://news.mit.edu/2021/programmable-fiber-0603},
abstract={Digital devices are the essential building blocks of any modern electronic system. Fibres containing digital devices could enable fabrics with digital system capabilities for applications in physiological monitoring, human-computer interfaces, and on-body machine-learning. Here, a scalable preform-to-fibre approach is used to produce tens of metres of flexible fibre containing hundreds of interspersed, digital temperature sensors and memory devices with a memory density of {\textasciitilde}7.6{\thinspace}{\texttimes}{\thinspace}105 bits per metre. The entire ensemble of devices are individually addressable and independently operated through a single connection at the fibre edge, overcoming the perennial single-fibre single-device limitation and increasing system reliability. The digital fibre, when incorporated within a shirt, collects and stores body temperature data over multiple days, and enables real-time inference of wearer activity with an accuracy of 96{\%} through a trained neural network with 1650 neuronal connections stored within the fibre. The ability to realise digital devices within a fibre strand which can not only measure and store physiological parameters, but also harbour the neural networks required to infer sensory data, presents intriguing opportunities for worn fabrics that sense, memorise, learn, and infer situational context.},
issn={2041-1723},
doi={10.1038/s41467-021-23628-5},
url={https://doi.org/10.1038/s41467-021-23628-5},
abbr={Nature Commun.},
paper={https://doi.org/10.1038/s41467-021-23628-5},
preview={fibers_icon.png},
selected=true
}
