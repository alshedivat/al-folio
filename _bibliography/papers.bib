---
---

@string{aps = {American Physical Society,}}

@article{laurencon2022the,
  abbr={NeurIPS Datasets},
  title={The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset},
  author={Hugo Lauren{\c{c}}on and Lucile Saulnier and Thomas Wang and Christopher Akiki and Albert Villanova del Moral and Teven Le Scao and Leandro Von Werra and Chenghao Mou and Eduardo Gonz{\'a}lez Ponferrada and Huu Nguyen and J{\"o}rg Frohberg and Mario {\v{S}}a{\v{s}}ko and Quentin Lhoest and Angelina McMillan-Major and G{\'e}rard Dupont and Stella Biderman and Anna Rogers and Loubna Ben allal and Francesco De Toni and Giada Pistilli and Olivier Nguyen and Somaieh Nikpoor and Maraim Masoud and Pierre Colombo and Javier de la Rosa and Paulo Villegas and Tristan Thrush and Shayne Longpre and Sebastian Nagel and Leon Weber and Manuel Romero Mu{\~n}oz and Jian Zhu and Daniel Van Strien and Zaid Alyafeai and Khalid Almubarak and Vu Minh Chien and Itziar Gonzalez-Dios and Aitor Soroa and Kyle Lo and Manan Dey and Pedro Ortiz Suarez and Aaron Gokaslan and Shamik Bose and David Ifeoluwa Adelani and Long Phan and Hieu Tran and Ian Yu and Suhas Pai and Jenny Chim and Violette Lepercq and Suzana Ilic and Margaret Mitchell and Sasha Luccioni and Yacine Jernite},
  abstract={As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus.},
  journal={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track,},
  publisher={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2022},
  url={https://openreview.net/forum?id=UoEw6KigkUn},
  html={https://openreview.net/forum?id=UoEw6KigkUn},
  pdf={bigscience_roots.pdf}
}

@article{Sharma2022HowSA,
  abbr={arXiv},
  title={How sensitive are translation systems to extra contexts? Mitigating gender bias in Neural Machine Translation models through relevant contexts},
  author={Shanya Sharma and Manan Dey and Koustuv Sinha},
  abstract={Neural Machine Translation systems built on top of Transformer-based architectures are routinely improving the state-of-the-art in translation quality according to word-overlap metrics. However, a growing number of studies also highlight the inherent gender bias that these models incorporate during training, which reflects poorly in their translations. In this work, we investigate whether these models can be instructed to fix their bias during inference using targeted, guided instructions as contexts. By translating relevant contextual sentences during inference along with the input, we observe large improvements in reducing the gender bias in translations, across three popular test suites (WinoMT, BUG, SimpleGen). We further propose a novel metric to assess several large pretrained models (OPUS-MT, M2M-100) on their sensitivity towards using contexts during translation to correct their biases. Our approach requires no fine-tuning, and thus can be used easily in production systems to de-bias translations from stereotypical gender-occupation bias. We hope our method, along with our metric, can be used to build better, bias-free translation systems.},
  journal={Findings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022,},
  publisher={Findings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2022},
  url={https://arxiv.org/abs/2205.10762},
  html={https://arxiv.org/abs/2205.10762},
  pdf={mt_bias.pdf}
}

@article{talat2022you,
  abbr={ACL Workshop},
  title={You reap what you sow: On the Challenges of Bias Evaluation Under Multilingual Settings},
  author={Zeerak Talat and Aur{\'e}lie N{\'e}v{\'e}ol and Stella Biderman and Miruna Clinciu and Manan Dey and Shayne Longpre and Sasha Luccioni and Maraim Masoud and Margaret Mitchell and Dragomir Radev and Shanya Sharma and Arjun Subramonian and Jaesung Tae and Samson Tan and Deepak Tunuguntla and Oskar van der Wal},
  abstract={Evaluating bias, fairness, and social impact in monolingual language models is a difficult task. This challenge is further compounded when language modeling occurs in a multilingual context. Considering the implication of evaluation biases for large multilingual language models, we situate the discussion of bias evaluation within a wider context of social scientific research with computational work. We highlight three dimensions of developing multilingual bias evaluation frameworks: (1) increasing transparency through documentation, (2) expanding targets of bias beyond gender, and (3) addressing cultural differences that exist between languages. We further discuss the power dynamics and consequences of training large language models and recommend that researchers remain cognizant of the ramifications of developing such technologies.},
  journal={Challenges & Perspectives in Creating Large Language Models workshop at ACL,},
  publisher={Challenges & Perspectives in Creating Large Language Models workshop at ACL},
  year={2022},
  url={https://openreview.net/forum?id=rK-7NhfSIW5},
  html={https://openreview.net/forum?id=rK-7NhfSIW5},
  pdf={bigscience_bias.pdf},
}

@article{sanh2022multitask,
  abbr={ICLR},
  title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author={Victor Sanh and Albert Webson and Colin Raffel and Stephen Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Teven Le Scao and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M Rush},
  abstract={Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown  et  al.,  2020). It has been hypothesized that this is a consequence of implicit multitask learning in language model training (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping general natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets,  each with multiple prompts using varying natural language. These prompted datasets allow for benchmarking the ability of a model  to  perform  completely  unseen tasks  specified in natural language.  We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance onseveral  datasets,  often  outperforming  models 16× its size.  Further, our model attains strong performance on a subset of tasks from the BIG-Bench benchmark, out-performing models 6× its size.},
  journal={International Conference on Learning Representations (Spotlight),},
  publisher={International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=9Vrb9D0WI4},
  html={https://openreview.net/forum?id=9Vrb9D0WI4},
  pdf={T0.pdf},
  selected={true}
}

@article{bach2022promptsource,
  abbr={ACL Demo Track},
  title={PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts},
  author={Stephen H. Bach and Victor Sanh and Zheng-Xin Yong and Albert Webson and Colin Raffel and Nihal V. Nayak and Abheesht Sharma and Taewoon Kim and M Saiful Bari and Thibault Fevry and Zaid Alyafeai and Manan Dey and Andrea Santilli and Zhiqing Sun and Srulik Ben-David and Canwen Xu and Gunjan Chhablani and Han Wang and Jason Alan Fries and Maged S. Al-shaibani and Shanya Sharma and Urmish Thakker and Khalid Almubarak and Xiangru Tang and Dragomir Radev and Mike Tian-Jian Jiang and Alexander M. Rush},
  abstract={PromptSource is a system for creating, sharing, and using natural language prompts. Prompts are functions that map an example from a dataset to a natural language input and target output. Using prompts to train and query language models is an emerging area in NLP that requires new tools that let users develop and refine these prompts collaboratively. PromptSource addresses the emergent challenges in this new setting with (1) a templating language for defining data-linked prompts, (2) an interface that lets users quickly iterate on prompt development by observing outputs of their prompts on many examples, and (3) a community-driven set of guidelines for contributing new prompts to a common pool. Over 2,000 prompts for roughly 170 datasets are already available in PromptSource.},
  journal={60th Annual Meeting of the Association for Computational Linguistics (ACL), Demo Track,},
  publisher={60th Annual Meeting of the Association for Computational Linguistics (ACL), Demo Track, 2022},
  year={2022},
  url={https://arxiv.org/abs/2202.01279},
  html={https://arxiv.org/abs/2202.01279},
  pdf={promptsource.pdf}
}

@article{mielke2021words,
  abbr={arXiv},
  title={Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP},
  author={Sabrina J. Mielke and Zaid Alyafeai and Elizabeth Salesky and Colin Raffel and Manan Dey and Matthias Gallé and Arun Raja and Chenglei Si and Wilson Y. Lee and Benoît Sagot and Samson Tan},
  abstract={What are the units of text that we want to model? From bytes to multi-word expressions, text can be analyzed and generated at many granularities. Until recently, most natural language processing (NLP) models operated over words, treating those as discrete and atomic tokens, but starting with byte-pair encoding (BPE), subword-based approaches have become dominant in many areas, enabling small vocabularies while still allowing for fast inference. Is the end of the road character-level model or byte-level processing? In this survey, we connect several lines of work from the pre-neural and neural era, by showing how hybrid approaches of words and characters as well as subword-based approaches based on learned segmentation have been proposed and evaluated. We conclude that there is and likely will never be a silver bullet singular solution for all applications and that thinking seriously about tokenization remains important for many applications.},
  journal={arXiv:2112.10508,},
  publisher={arXiv:2112.10508},
  year={2021},
  url={https://arxiv.org/abs/2112.10508},
  html={https://arxiv.org/abs/2112.10508},
  pdf={tokenization.pdf}
}

@article{sharma2021evaluating,
  abbr={NeurIPS DCS},
  title={Evaluating Gender Bias in Natural Language Inference},
  author={Shanya Sharma and Manan Dey and Koustuv Sinha},
  abstract={Gender-bias stereotypes have recently raised significant ethical concerns in natural language processing. However, progress in detection and evaluation of gender bias in natural language understanding through inference is limited and requires further investigation. In this work, we propose an evaluation methodology to measure these biases by constructing a challenge task that involves pairing gender-neutral premises against a gender-specific hypothesis. We use our challenge task to investigate state-of-the-art NLI models on the presence of gender stereotypes using occupations. Our findings suggest that three models (BERT, RoBERTa, BART) trained on MNLI and SNLI datasets are significantly prone to gender-induced prediction errors. We also find that debiasing techniques such as augmenting the training dataset to ensure a gender-balanced dataset can help reduce such bias in certain cases.},
  journal={Workshop on Dataset Curation and Security at NeurIPS,},
  publisher={NeurIPS 2020 Workshop on Dataset Curation and Security},
  year={2020},
  url={http://securedata.lol/camera_ready/19.pdf},
  html={http://securedata.lol/camera_ready/19.pdf},
  pdf={gender_bias_nli.pdf}
}

@article{sharma2020assessing,
  abbr={NeurIPS AI4SG},
  title={Assessing Viewer's Mental Health by Detecting Depression in YouTube Videos},
  author={Shanya Sharma and Manan Dey},
  abstract={Depression is one of the most prevalent mental health issues around the world, proving to be one of the leading causes of suicide and placing large economic burdens on families and society. In this paper, we develop and test the efficacy of machine learning techniques applied to the content of YouTube videos captured through their transcripts and determine if the videos are depressive or have a depressing trigger. Our model can detect depressive videos with an accuracy of 83%. We also introduce a real-life evaluation technique to validate our classification based on the comments posted on a video by calculating the CES-D scores of the comments. This work conforms greatly with the UN Sustainable Goal of ensuring Good Health and Well Being with major conformity with section UN SDG 3.4.},
  journal={AI for Social Good workshop at NeurIPS,},
  publisher={AI for Social Good workshop at NeurIPS},
  year={2019},
  url={https://aiforsocialgood.github.io/neurips2019/accepted/track1/pdfs/52_aisg_neurips2019.pdf},
  html={https://aiforsocialgood.github.io/neurips2019/accepted/track1/pdfs/52_aisg_neurips2019.pdf},
  pdf={depression.pdf}
}