@STRING{UNDERREVIEW = {under review}}
@STRING{JIMAGING = {Journal of Imaging}}
@STRING{JIMAGING_SHORT = {J. Imaging}}
@STRING{CANCERS = {Cancers}}
@STRING{CANCERS_SHORT = {Cancers}}
@STRING{OSF = {Open Science Foundation}}
@STRING{WACV = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}}
@STRING{WACV_SHORT = {WACV}}
@STRING{BIOIMAGES = {BioImage Archive}}
@STRING{DATA = {Data}}
@STRING{DATA_SHORT = {Data}}

@misc{ferber2023large,
  title     = {Large Language Models for Information Retrieval and Comparison of Medical Oncology Guidelines},
  author    = {Ferber, Dyke and Wiest, Isabella C. and Juglan, Radhika and W\"{o}lflein, Georg and Carrero, Zunamys I. and Ebert, Matthias P. and Beutel, Gernot and Eckardt, Jan-Niklas and Meddeke, Jan Moritz and Truhn, Daniel and J\"{a}ger, Dirk and Kather, Jakob Nikolas},
  author+an = {4=highlight},
  year      = {2023},
  note      = UNDERREVIEW,
  keywords  = {preprint}
}

@misc{elnahhas2023from,
  title     = {From Whole-slide Image to Biomarker Prediction: A Protocol for End-to-End Deep Learning in Computational Pathology},
  author    = {El Nahhas, Omar S. M. and van Treeck, Marko and W\"{o}lflein, Georg and Unger, Michaela and Ligero, Marta and Lenz, Tim and Wagner, Sophia J. and Hewitt, Katherine J. and Khader, Firas and Foersch, Sebastian and Truhn, Daniel and Kather, Jakob Nikolas},
  author+an = {3=highlight},
  year      = {2023},
  note      = UNDERREVIEW,
  url       = {https://arxiv.org/abs/2312.10944},
  pdf       = {https://arxiv.org/pdf/2312.10944.pdf},
  code      = {https://github.com/KatherLab/STAMP},
  preview   = {stamp.png},
  abstract  = {Hematoxylin- and eosin (H\&E) stained whole-slide images (WSIs) are the foundation of diagnosis of cancer. In recent years, development of deep learning-based methods in computational pathology enabled the prediction of biomarkers directly from WSIs. However, accurately linking tissue phenotype to biomarkers at scale remains a crucial challenge for democratizing complex biomarkers in precision oncology. This protocol describes a practical workflow for solid tumor associative modeling in pathology (STAMP), enabling prediction of biomarkers directly from WSIs using deep learning. The STAMP workflow is biomarker agnostic and allows for genetic- and clinicopathologic tabular data to be included as an additional input, together with histopathology images. The protocol consists of five main stages which have been successfully applied to various research problems: formal problem definition, data preprocessing, modeling, evaluation and clinical translation. The STAMP workflow differentiates itself through its focus on serving as a collaborative framework that can be used by clinicians and engineers alike for setting up research projects in the field of computational pathology. As an example task, we applied STAMP to the prediction of microsatellite instability (MSI) status in colorectal cancer, showing accurate performance for the identification of MSI-high tumors. Moreover, we provide an open-source codebase which has been deployed at several hospitals across the globe to set up computational pathology workflows. The STAMP workflow requires one workday of hands-on computational execution and basic command line knowledge.},
  keywords  = {preprint}
}

@misc{wolflein2023good,
  title     = {A Good Feature Extractor Is All You Need for Weakly Supervised Learning in Histopathology},
  author    = {W\"{o}lflein, Georg and Ferber, Dyke and Meneghetti, Asier Rabasco and El Nahhas, Omar S. M. and Truhn, Daniel and Carrero, Zunamys I. and Harrison, David J. and Arandjelovi\'{c}, Ognjen and Kather, Jakob N.},
  author+an = {1=highlight},
  year      = {2023},
  note      = UNDERREVIEW,
  selected  = {true},
  url       = {https://arxiv.org/abs/2311.11772},
  pdf       = {https://arxiv.org/pdf/2311.11772.pdf},
  code      = {https://github.com/georg-wolflein/good-features},
  website   = {https://georg.woelflein.eu/good-features},
  video     = {https://www.youtube.com/watch?v=Tst4XtaT9RE},
  preview   = {histaug.png},
  abstract  = {Deep learning is revolutionising pathology, offering novel opportunities in disease prognosis and personalised treatment. Historically, stain normalisation has been a crucial preprocessing step in computational pathology pipelines, and persists into the deep learning era. Yet, with the emergence of feature extractors trained using self-supervised learning (SSL) on diverse pathology datasets, we call this practice into question. In an empirical evaluation of publicly available feature extractors, we find that omitting stain normalisation and image augmentations does not compromise downstream performance, while incurring substantial savings in memory and compute. Further, we show that the top-performing feature extractors are remarkably robust to variations in stain and augmentations like rotation in their latent space. Contrary to previous patch-level benchmarking studies, our approach emphasises clinical relevance by focusing on slide-level prediction tasks in a weakly supervised setting with external validation cohorts. This work represents the most comprehensive robustness evaluation of public pathology SSL feature extractors to date, involving more than 6,000 training runs across nine tasks, five datasets, three downstream architectures, and various preprocessing setups. Our findings stand to streamline digital pathology workflows by minimising preprocessing needs and informing the selection of feature extractors.},
  keywords  = {preprint}
}

@misc{wolflein2023deep,
  title     = {Deep Multiple Instance Learning with Distance-Aware Self-Attention},
  author    = {W\"{o}lflein, Georg and Magister, Lucie Charlotte and Li\`{o}, Pietro and Harrison, David J and Arandjelovi\'{c}, Ognjen},
  author+an = {1=highlight},
  year      = {2023},
  note      = UNDERREVIEW,
  selected  = {true},
  url       = {https://arxiv.org/abs/2305.10552},
  pdf       = {https://arxiv.org/pdf/2305.10552.pdf},
  code      = {https://github.com/georg-wolflein/das-mil},
  preview   = {das-mil.png},
  abstract  = {Traditional supervised learning tasks require a label for every instance in the training set, but in many real-world applications, labels are only available for collections (bags) of instances. This problem setting, known as multiple instance learning (MIL), is particularly relevant in the medical domain, where high-resolution images are split into smaller patches, but labels apply to the image as a whole. Recent MIL models are able to capture correspondences between patches by employing self-attention, allowing them to weigh each patch differently based on all other patches in the bag. However, these approaches still do not consider the relative spatial relationships between patches within the larger image, which is especially important in computational pathology. To this end, we introduce a novel MIL model with distance-aware self-attention (DAS-MIL), which explicitly takes into account relative spatial information when modelling the interactions between patches. Unlike existing relative position representations for self-attention which are discrete, our approach introduces continuous distance-dependent terms into the computation of the attention weights, and is the first to apply relative position representations in the context of MIL. We evaluate our model on a custom MNIST-based MIL dataset that requires the consideration of relative spatial information, as well as on CAMELYON16, a publicly available cancer metastasis detection dataset, where we achieve a test AUROC score of 0.91. On both datasets, our model outperforms existing MIL approaches that employ absolute positional encodings, as well as existing relative position representation schemes applied to MIL. Our code is available at https://anonymous.4open.science/r/das-mil.},
  keywords  = {preprint}
}

@misc{alouges2023performance,
  title        = {Performance comparison between federated and centralized learning with a deep learning model on {Hoechst} stained images},
  author       = {Alouges, Damien and W\"olflein, Georg and Um, In Hwa and Harrison, David and Arandjelovi\'c, Ognjen and Battail, Christophe and Gazut, St\'ephane},
  author+an    = {2=highlight},
  howpublished = {ISMB/ECCB Abstracts},
  year         = {2023},
  month        = {7},
  preview      = {hoechstgan.png},
  abbr         = {ISMB/ECCB},
  abstract     = {Medical data is not fully exploited by Machine Learning (ML) techniques because the privacy concerns restrict the sharing of sensitive information and consequently the use of centralized ML schemes. Usually, ML models trained on local data are failing to reach their full potential owing to low statistical power. Federated Learning (FL) solves critical issues in the healthcare domain such as data privacy and enables multiple contributors to build a common and robust ML model by sharing local learning parameters without sharing data. FL approaches are mainly evaluated in the literature using benchmarks and the trade-off between accuracy and privacy still has to be more studied in a realistic clinical context. In this work, part of the European project KATY (GA:101017453), we evaluate this trade-off for a CD3/CD8 cells staining procedure from Hoechst images. Wölflein et al. developed a deep learning GAN model that synthesizes CD3 and CD8 stains from kidney cancer tissue slides, trained on 473,000 patches (256x256 pixels) from 8 whole slide images. We modified the training to simulate a FL approach by distributing the learning across 8 clients and aggregating the parameters to create the overall model. We present then the performance comparison between FL and centralized learning.},
  keywords     = {abstract},
  url          = {https://zenodo.org/records/8214502},
  pdf          = {https://zenodo.org/records/8214502/files/Alouges_ECCB-ISMB2023.pdf}
}

@article{wolflein2023data,
  title          = {Whole-Slide Images and Patches of Clear Cell Renal Cell Carcinoma Tissue Sections Counterstained with {Hoechst} 33342, {CD3}, and {CD8} Using Multiple Immunofluorescence},
  author         = {W\"{o}lflein, Georg and Um, In Hwa and Harrison, David J and Arandjelovi\'{c}, Ognjen},
  author+an      = {1=first, highlight; 2=first},
  firstauthors   = {2},
  journal        = DATA,
  abbr           = DATA_SHORT,
  volume         = {8},
  year           = {2023},
  month          = {2},
  number         = {2},
  article-number = {40},
  url            = {https://www.mdpi.com/2306-5729/8/2/40},
  doi            = {10.3390/data8020040},
  keywords       = {journal,paper},
  abstract       = {In recent years, there has been an increased effort to digitise whole-slide images of cancer tissue. This effort has opened up a range of new avenues for the application of deep learning in oncology. One such avenue is virtual staining, where a deep learning model is tasked with reproducing the appearance of stained tissue sections, conditioned on a different, often times less expensive, input stain. However, data to train such models in a supervised manner where the input and output stains are aligned on the same tissue sections are scarce. In this work, we introduce a dataset of ten whole-slide images of clear cell renal cell carcinoma tissue sections counterstained with Hoechst 33342, CD3, and CD8 using multiple immunofluorescence. We also provide a set of over 600,000 patches of size 256 × 256 pixels extracted from these images together with cell segmentation masks in a format amenable to training deep learning models. It is our hope that this dataset will be used to further the development of deep learning methods for digital pathology by serving as a dataset for comparing and benchmarking virtual staining models.},
  preview        = {hoechstgan-dataset.png},
  pdf            = {https://www.mdpi.com/2306-5729/8/2/40/pdf}
}

@inproceedings{woelflein2023wacv,
  title     = {{HoechstGAN}: Virtual Lymphocyte Staining Using Generative Adversarial Networks},
  author    = {W\"{o}lflein, Georg and Um, In Hwa and Harrison, David J and Arandjelovi\'{c}, Ognjen},
  author+an = {1=highlight},
  booktitle = WACV,
  abbr      = WACV_SHORT,
  arxiv     = {2210.06909},
  month     = {1},
  year      = {2023},
  selected  = {true},
  website   = {https://georg.woelflein.eu/hoechstgan},
  url       = {https://openaccess.thecvf.com/content/WACV2023/html/Wolflein_HoechstGAN_Virtual_Lymphocyte_Staining_Using_Generative_Adversarial_Networks_WACV_2023_paper.html},
  pdf       = {https://openaccess.thecvf.com/content/WACV2023/papers/Wolflein_HoechstGAN_Virtual_Lymphocyte_Staining_Using_Generative_Adversarial_Networks_WACV_2023_paper.pdf},
  preview   = {hoechstgan.png},
  poster    = {hoechstgan_poster.pdf},
  video     = {https://www.youtube.com/watch?v=XrhwqRNjtv4},
  code      = {https://github.com/georg-wolflein/hoechstgan},
  pages     = {4997--5007},
  abstract  = {The presence and density of specific types of immune cells are important to understand a patient's immune response to cancer. However, immunofluorescence staining required to identify T cell subtypes is expensive, timeconsuming, and rarely performed in clinical settings. We present a framework to virtually stain Hoechst images (which are cheap and widespread) with both CD3 and CD8 to identify T cell subtypes in clear cell renal cell carcinoma using generative adversarial networks. Our proposed method jointly learns both staining tasks, incentivising the network to incorporate mutually beneficial information from each task. We devise a novel metric to quantify the virtual staining quality, and use it to evaluate our method.},
  keywords  = {conference,paper}
}

@misc{wolflein2022bioimages,
  author       = {W\"{o}lflein, Georg and Um, In Hwa and Harrison, David J and Arandjelovi\'{c}, Ognjen},
  author+an    = {1=highlight},
  title        = {{Whole slide images and patches of clear cell renal cell carcinoma counterstained with multiple immunofluorescence for {Hoechst}, {CD3}, and {CD8}}},
  year         = {2022},
  month        = {12},
  howpublished = BIOIMAGES,
  url          = {https://www.ebi.ac.uk/biostudies/bioimages/studies/S-BIAD605},
  website      = {https://www.ebi.ac.uk/biostudies/bioimages/studies/S-BIAD605},
  keywords     = {dataset},
  preview      = {hoechstgan-dataset.png},
  abstract     = {We provide a dataset of 10 whole slide images of clear cell renal cell carcinoma tissue sections that were counterstained with Hoechst 33342, Cy3 and Cy5. The fluorescent images were captured using Zeiss Axio Scan Z1. We used three different fluorescent channels (Hoechst 33342, Cy3 and Cy5) simultaneously to capture individual channel images under 20x object magnification with respective exposure times of 10 ms, 20 ms and 30 ms. We also provide clinical data including age at surgery, gender, disease free months, nuclear grade, and Leibovich score. Alongside the WSIs, we provide pre-processed and normalised patches (each 256x256 pixels) from the WSIs in a format that can be readily ingested by deep learning models for their training. For each patch, we provide normalised Hoechst, CD3, and CD8 images, as well as cell masks identified using the StarDist algorithm. Each cell is annotated as CD3, CD8 (a subset of CD3), or neither.}
}

@article{defilippis2022,
  title          = {Use of High-Plex Data Reveals Novel Insights into the Tumour Microenvironment of Clear Cell Renal Cell Carcinoma},
  author         = {De Filippis, Raffaele and W\"{o}lflein, Georg and Um, In Hwa and Caie, Peter D and Warren, Sarah and White, Andrew and Suen, Elizabeth and To, Emily and Arandjelovi\'{c}, Ognjen and Harrison, David J},
  author+an      = {1=first; 2=first, highlight},
  firstauthors   = {2},
  journal        = CANCERS,
  abbr           = CANCERS_SHORT,
  volume         = {14},
  year           = {2022},
  month          = {11},
  number         = {21},
  article-number = {5387},
  url            = {https://www.mdpi.com/2072-6694/14/21/5387},
  pdf            = {https://www.mdpi.com/2072-6694/14/21/5387/pdf},
  abstract       = {Although immune checkpoint inhibitors (ICIs) have significantly improved the oncological outcomes, about one-third of patients affected by clear cell renal cell carcinoma (ccRCC) still experience recurrence. Current prognostic algorithms, such as the Leibovich score (LS), rely on morphological features manually assessed by pathologists and are therefore subject to bias. Moreover, these tools do not consider the heterogeneous molecular milieu present in the Tumour Microenvironment (TME), which may have prognostic value. We systematically developed a semi-automated method to investigate 62 markers and their combinations in 150 primary ccRCCs using Multiplex Immunofluorescence (mIF), NanoString GeoMx&reg; Digital Spatial Profiling (DSP) and Artificial Intelligence (AI)-assisted image analysis in order to find novel prognostic signatures and investigate their spatial relationship. We found that coexpression of cancer stem cell (CSC) and epithelial-to-mesenchymal transition (EMT) markers such as OCT4 and ZEB1 are indicative of poor outcome. OCT4 and the immune markers CD8, CD34, and CD163 significantly stratified patients at intermediate LS. Furthermore, augmenting the LS with OCT4 and CD34 improved patient stratification by outcome. Our results support the hypothesis that combining molecular markers has prognostic value and can be integrated with morphological features to improve risk stratification and personalised therapy. To conclude, GeoMx&reg; DSP and AI image analysis are complementary tools providing high multiplexing capability required to investigate the TME of ccRCC, while reducing observer bias.},
  doi            = {10.3390/cancers14215387},
  medrxiv        = {10.1101/2022.10.13.22281035},
  preview        = {highplex.png},
  keywords       = {journal,paper}
}

@article{wolflein2021jimaging,
  title          = {Determining Chess Game State from an Image},
  author         = {W\"olflein, Georg and Arandjelovi\'c, Ognjen},
  author+an      = {1=highlight},
  journal        = JIMAGING,
  abbr           = JIMAGING_SHORT,
  volume         = {7},
  year           = {2021},
  month          = {6},
  number         = {6},
  article-number = {94},
  url            = {https://www.mdpi.com/2313-433X/7/6/94},
  arxiv          = {2104.14963},
  doi            = {10.3390/jimaging7060094},
  keywords       = {journal,paper},
  code           = {https://github.com/georg-wolflein/chesscog},
  abstract       = {Identifying the configuration of chess pieces from an image of a chessboard is a problem in computer vision that has not yet been solved accurately. However, it is important for helping amateur chess players improve their games by facilitating automatic computer analysis without the overhead of manually entering the pieces. Current approaches are limited by the lack of large datasets and are not designed to adapt to unseen chess sets. This paper puts forth a new dataset synthesised from a 3D model that is an order of magnitude larger than existing ones. Trained on this dataset, a novel end-to-end chess recognition system is presented that combines traditional computer vision techniques with deep learning. It localises the chessboard using a RANSAC-based algorithm that computes a projective transformation of the board onto a regular grid. Using two convolutional neural networks, it then predicts an occupancy mask for the squares in the warped image and finally classifies the pieces. The described system achieves an error rate of 0.23\% per square on the test set, 28 times better than the current state of the art. Further, a few-shot transfer learning approach is developed that is able to adapt the inference system to a previously unseen chess set using just two photos of the starting position, obtaining a per-square accuracy of 99.83\% on images of that new chess set. The code, dataset, and trained models are made available online.},
  preview        = {chesscog.png},
  pdf            = {https://www.mdpi.com/2313-433X/7/6/94/pdf},
  blog           = {/blog/2021/chesscog},
  selected       = {true}
}

@misc{wolflein2021osf,
  title        = {Dataset of Rendered Chess Game State Images},
  author       = {W\"olflein, Georg and Arandjelovi\'c, Ognjen},
  author+an    = {1=highlight},
  url          = {https://osf.io/xf3ka},
  website      = {https://osf.io/xf3ka},
  doi          = {10.17605/OSF.IO/XF3KA},
  howpublished = OSF,
  year         = {2021},
  month        = {5},
  keywords     = {dataset},
  preview      = {chesscog-dataset.png},
  abstract     = {This dataset contains 4,888 synthetic images of chess game states that occurred in games played by Magnus Carlsen. The images were rendered in Blender at different angles and lighting conditions.}
}

@thesis{wolflein2021msci,
  title       = {Determining Chess Game State From an Image Using Machine Learning},
  author      = {W\"olflein, Georg},
  author+an   = {1=highlight},
  advisor     = {Arandjelovi\'c, Ognjen},
  institution = {University of St Andrews},
  year        = {2021},
  month       = {1},
  badge       = {MSci},
  preview     = {chesscog.png},
  pdf         = {https://github.com/georg-wolflein/chesscog-report/raw/master/report.pdf},
  code        = {https://github.com/georg-wolflein/chesscog},
  url         = {https://github.com/georg-wolflein/chesscog},
  blog        = {/blog/2021/chesscog},
  abstract    = {Identifying the configuration of chess pieces from an image of a chessboard is a problem in computer vision that has not yet been solved accurately. However, it is important for helping amateur chess players improve their games by facilitating automatic computer analysis without the overhead of manually entering the pieces. Current approaches are limited by the lack of large datasets and are not designed to adapt to unseen chess sets. This project puts forth a new dataset synthesised from a 3D model that is two orders of magnitude larger than existing ones. Trained on this dataset, a novel end-to-end chess recognition system is presented that combines traditional computer vision techniques with deep learning. It localises the chessboard using a RANSAC-based algorithm that computes a projective transformation of the board onto a regular grid. Using two convolutional neural networks, it then predicts an occupancy mask for the squares in the warped image and finally classifies the pieces. The described system achieves an error rate of 0.23\% per square on the test set, 28 times better than the current state of the art. Further, a one-shot transfer learning approach is developed that is able to adapt the inference system to a previously unseen chess set using just two photos of the starting position, obtaining a per-square accuracy of 99.83\% on images of that new chess set. Inference takes less than half a second on a GPU and about two seconds on a CPU. The feasibility of the system is demonstrated in an interactive web app available at https://www.chesscog.com.}
}

@thesis{wolflein2020bsc,
  title       = {Freeing Neural Training Through Surfing},
  author      = {W\"olflein, Georg},
  author+an   = {1=highlight},
  advisor     = {Weir, Michael},
  institution = {University of St Andrews},
  year        = {2020},
  month       = {4},
  badge       = {BSc},
  preview     = {neural-surfing.png},
  pdf         = {https://github.com/georg-wolflein/neural-surfing/raw/master/report/report.pdf},
  code        = {https://github.com/georg-wolflein/neural-surfing},
  url         = {https://github.com/georg-wolflein/neural-surfing},
  abstract    = {Gradient methods based on backpropagation are widely used in training multilayer feedforward neural networks. However, such algorithms often converge to suboptimal weight configurations known as local minima. This report presents a novel minimal example of the local minimum problem with only three training samples and demonstrates its suitability for investigating and resolving said problem by analysing its mathematical properties and conditions leading to the failure of conventional training regimes. A different perspective for training neural networks is introduced that concerns itself with neural spaces and is applied to study the local minimum example.This gives rise to the concept of setting intermediate subgoals during training which is demonstrated to be a viable and effective means of overcoming the local minimum problem. The versatility of subgoal-based approaches is highlighted by showing their potential for training more generally. An example of a subgoal-based training regime using sampling and an adaptive clothoid for establishing a goal-connecting path is suggested as a proof of concept for further research. In addition, this project includes the design and implementation of a software framework for monitoring the performance of different neural training algorithms on a given problem simultaneously and in real time. This framework can be used to reproduce the findings of how classical algorithms fail to find the global minimum in the aforementioned example.}
}