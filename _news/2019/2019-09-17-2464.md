---
layout: post
title: Preliminary Examination Announcement&#58; Multi-view, multi-modal speech+gesture interaction in large display environments
date: 2019-09-17
tags: event
categories: events
tabs: true
image: jaurisano_prelim.jpg-srcw.jpg
---

Ph.D. Student: Jillian Aurisano<br><br>
Date: Tuesday, September 17, 2019<br>
Time: 11:00 am <br>
Location: Room 3036, Engineering Research Facility<br><br>
Committee:<br>
Andrew Johnson (Chair)<br>
Debaleena Chattopadhyay<br>
Barbara Di Eugenio<br>
G. Elisabeta Marai<br>
Rick Stevens (Argonne National Laboratory, University of Chicago)<br><br>
Abstract:<br> 
Analysis of large, complex datasets stands to benefit from environments that permit users to view and juxtapose many views of data.  Large, high-resolution environments are capable of showing many related views of data, but interaction with these views poses significant challenges in visual and interaction design.  In this talk, I will present work toward &ldquo;multi-view interactions&rdqo; that enable users to create, organize and act on many views at once, through multi-modal speech+gesture queries in large flexible canvas environments.  The goal is the enable users to rapidly and efficiently generate sets of views in support of multi-view analysis tasks, organize these views to reflect changing analysis goals, and operate on sets of views collectively, rather than individually, to efficiently reach large portions of the &lsquo;data+attribute space&rsquo;.  I will implement and evaluate this approach within two domains and two user communities: City of Chicago data for informed citizens and deep learning predictions data for precision medicine in cancer.

![image](/images/jaurisano_prelim.jpg-srcw.jpg){:style="max-width: 100%"}
