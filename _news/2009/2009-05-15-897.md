---
layout: post
title: 'The Next Best Thing to You'
date: 2009-05-15
tags: event
categories: events
tabs: true
---

<a href="http://www.nsf.gov/news/news_summ.jsp?cntn_id=114828&amp;govDel=USNSF_51">Link to the full NSF press release</a><br>
Also posted to <a href="http://www.usnews.com/articles/science/2009/05/20/the-next-best-thing-to-you.html">U.S. News &amp; World Report</a><br><br>

<strong>May 15, 2009</strong><br><br>

<strong><em>New avatar technology combines advances in artificial intelligence and computer image rendering</em></strong><br><br>

Have you ever wished you could be in two places at once? Perhaps you&rsquo;ve had the desire to create a copy of yourself that could stand in for you at a meeting, freeing you up to work on more pressing matters. Thanks to a research project called LifeLike, that fantasy might be a little closer to reality.<br><br>

Project LifeLike is a collaboration between the Intelligent Systems Laboratory (ISL) at the University of Central Florida (UCF) and the Electronic Visualization Laboratory (EVL) at the University of Illinois at Chicago (UIC) that aims to create visualizations of people, or avatars, that are as realistic as possible. While their current results are far from perfect replications of a specific person, their work has advanced the field forward and opens up a host of possible applications in the not-too-distant future.<br><br>

The EVL team, headed by Jason Leigh, an associate professor of computer science, is tasked with getting the visual aspects of the avatar just right. On the surface, this seems like a pretty straightforward task - anyone who has played a video game that features characters from movies or professional athletes is used to computer-generated images that look like real people.<br><br>

But according to Leigh, it takes more than a good visual rendering to make an avatar truly seem like a human being. &ldquo;Visual realism is tough,&rdquo; Leigh said in a recent interview. &ldquo;Research shows that over 70% of communication is non-verbal,&rdquo; he said, and is dependent on subtle gestures, variations in a person&rsquo;s voice and other variables.<br><br>

To get these non-verbal aspects right, the EVL team has to take precise 3-D measurements of the person that Project LifeLike seeks to copy, capturing the way their face moves and other body language so the program can replicate those fine details later.
Avatar of Alex Schwarzkopf (NSF)

![image](https://www.evl.uic.edu/output/originals/lifelike_20090529.jpg-srcw.jpg){:style="max-width: 100%"}
Credit: S. Lee, EVL

