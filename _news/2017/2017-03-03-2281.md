---
layout: post
title: 'PhD Defense Announcement: "Multiview Immersion in Hybrid Reality Environments"'
date: 2017-03-03
tags: event
categories: events
tabs: true
---

Alessandro Febretti presents his PhD research in developing a model of an operating system for Hybrid Reality Environments, called Omegalib.<br><br>
Friday, March 3, 2017, 2pm<br>
EVL Cybercommons - 2068 ERF<br><br>
Committee: Andrew E. Johnson (Chair), Robert B. Kenyon, Luc Renambot, Leilah B. Lyons, Michael E. Papka (Argonne National Lab)<br><br>
Abstract:<br>
In the domain of large-scale visualization instruments, Hybrid Reality Environments (HREs) are a recent innovation that combines the best-in-class capabilities of immersive environments, with the best-in-class capabilities of ultra-high-resolution display walls. HREs create a seamless 2D/3D environment that supports both information-rich analysis as well as virtual-reality simulation exploration at a resolution matching human visual acuity. Collaborative research groups in HREs tend to work on a variety of tasks during a research session (sometimes in parallel), and these tasks require 2D data views, 3D views, linking between them, and the ability to bring in (or hide) data quickly as needed. Addressing these needs requires a matching software infrastructure that fully leverages the technological affordances offered by these new instruments.<br><br>
In this dissertation, I detail the requirements of such infrastructure and outline the model of an operating system for Hybrid Reality Environments. I present an implementation of core components of this model, called Omegalib. I show how this implementation has been successfully used to create HRE visualizations for research, education and outreach. One key feature of Omegalib is its ability to support multiple applications running simultaneously on an HRE. This is a significant improvement compared to classic immersive systems, which normally support a single application at a time. HREs (and large displays in general) are ideal systems to support co-located collaboration, and the ability to run and control multi-application workspaces is key to effective work in this setting. However, enabling multiple immersive applications on a large display presents several challenges. I discuss my solution to these challenges and present an extension of Omegalib that supports fully dynamic, multi-view, multi-user immersion. I evaluate this new system, which I call Multiview Immersion (MVI) in a formal user study: two-user groups are asked to compare 3D sonar scans using MVI, single-view immersion and multiple views without immersion (simulating a standard display wall). My objective is understanding the effect of MVI on analysis effectiveness, view usage and collaboration patterns compared to alternatives. MVI appears to reduce analysis error rates for our sample task and makes it more likely for both users to remain engaged with the analysis task.

![image](https://www.evl.uic.edu/output/originals/afebrettithesis_omegalib.png-srcw.jpg){:style="max-width: 100%"}
Credit: A. Febretti, EVL

