---
layout: post
title: 'Energy Efficiency of LLM Inference Across Various AI Accelerators (poster)'
date: 2025-05-08
tags: paper
categories: papers
tabs: true
image: energyefficiencyllm-2.png-srcw.jpg
---

## Energy Efficiency of LLM Inference Across Various AI Accelerators (poster)
**Brunetta, G., Lan, Z., Wu, X., Taylor, V.**
- Location: Chicago, IL
- Link: [https://gcasr.org/2025/posters](https://gcasr.org/2025/posters)
- PDF: [energyefficiencyllm.pdf](/documents/energyefficiencyllm.pdf)

[![image](/images/energyefficiencyllm-2.png-srcw.jpg){:style="max-width: 100%"}](/images/energyefficiencyllm-2.png-srcw.jpg)

In recent years, numerous hardware accelerators have been developed to meet the rising demand for machine learning (ML) workloads, and Large Language Models inference in particular. GPUs are currently the standard for ML training and inference. However, they require substantial data movements that hurt performance and increase power consumption, making systems extremely energy-intensive. In response, many companies, including Intel, AMD, and Google, as well as numerous startups such as Groq, SambaNova, Cerebras, and Graphcore, have introduced specialized accelerators for ML workloads that leverage a dataflow design, which aims to reduce data movement and thus improve both performance and power consumption. This article presents a comparative analysis of the performance and energy efficiency of various AI accelerators and GPUs for large language model (LLM) inference, using popular open-source models evaluated on both synthetic and real-world datasets.
